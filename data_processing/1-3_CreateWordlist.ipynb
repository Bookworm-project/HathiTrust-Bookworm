{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Wordlist\n",
    "\n",
    "Create a vocabulary list for Bookworm, optimized to include top language-specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.959613915532827"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "st = os.stat('/notebooks/data2/final/final-sorted.h5')\n",
    "st.st_size / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/eng    79005095\n",
       "/ger    30446373\n",
       "/fre    17440715\n",
       "/lat    13691932\n",
       "/rus    10851839\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all langs and their sizes\n",
    "with pd.HDFStore('/notebooks/data2/final/final-sorted.h5') as store:\n",
    "    keys = store.keys()\n",
    "    sizes = [store.get_storer(key).nrows for key in keys]\n",
    "tablesizes = pd.Series(sizes, index=keys).sort_values(ascending=False)\n",
    "tablesizes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2230249, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pd.HDFStore('/notebooks/data2/final/final-sorted.h5') as store:\n",
    "    df = store.select('/gre')\n",
    "df[df.index.str.startswith('Ελλάδα')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ελλάδα</th>\n",
       "      <td>302969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδας</th>\n",
       "      <td>53219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδα»</th>\n",
       "      <td>4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδας»</th>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδα...</th>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδα\"</th>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδα*</th>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδας-Τουρκίας</th>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδαδ</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδα...»</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδα]</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδα1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδας\"</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδα-Τουρκία</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδας-Βουλγαρίας</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδας...</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδας-ΕΟΚ</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ελλάδαν</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count\n",
       "token                     \n",
       "Ελλάδα              302969\n",
       "Ελλάδας              53219\n",
       "Ελλάδα»               4525\n",
       "Ελλάδας»               771\n",
       "Ελλάδα...              507\n",
       "Ελλάδα\"                313\n",
       "Ελλάδα*                162\n",
       "Ελλάδας-Τουρκίας       107\n",
       "Ελλάδαδ                102\n",
       "Ελλάδα...»              80\n",
       "Ελλάδα]                 53\n",
       "Ελλάδα1                 48\n",
       "Ελλάδας\"                32\n",
       "Ελλάδα-Τουρκία          25\n",
       "Ελλάδας-Βουλγαρίας      14\n",
       "Ελλάδας...              13\n",
       "Ελλάδας-ΕΟΚ             13\n",
       "Ελλάδαν                 10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index.str.startswith('Ελλάδα')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine a trimming policy for each lang\n",
    "\n",
    "Each language contributes top $N_{lang}$ tokens to the word list. $N_{lang}$ is selected according to the following rules:\n",
    "\n",
    "    4% of the language's saved vocabulary, to a minimum of 25k, and hard-coded adjustments for the biggest languages where 4% is too high (eng=900m, ger=650k, {fre,lat,rus}=400k, {jpn,ita,spa}=250k). Any language with less than 100k tokens *total* is assumed to be a junk language, or one that BW is not useful for to begin with, so it's trimmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens (including possible dupes) 6588484\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>count</th>\n",
       "      <th>retain_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/eng</td>\n",
       "      <td>79005095</td>\n",
       "      <td>900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/ger</td>\n",
       "      <td>30446373</td>\n",
       "      <td>650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/fre</td>\n",
       "      <td>17440715</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/lat</td>\n",
       "      <td>13691932</td>\n",
       "      <td>360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/rus</td>\n",
       "      <td>10851839</td>\n",
       "      <td>280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/jpn</td>\n",
       "      <td>8333906</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/ita</td>\n",
       "      <td>7069154</td>\n",
       "      <td>220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/spa</td>\n",
       "      <td>7027856</td>\n",
       "      <td>220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/chi</td>\n",
       "      <td>5210120</td>\n",
       "      <td>182354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/und</td>\n",
       "      <td>4886094</td>\n",
       "      <td>171013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lang     count  retain_count\n",
       "0  /eng  79005095        900000\n",
       "1  /ger  30446373        650000\n",
       "2  /fre  17440715        400000\n",
       "3  /lat  13691932        360000\n",
       "4  /rus  10851839        280000\n",
       "5  /jpn   8333906        300000\n",
       "6  /ita   7069154        220000\n",
       "7  /spa   7027856        220000\n",
       "8  /chi   5210120        182354\n",
       "9  /und   4886094        171013"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference for how many top words to keep\n",
    "top_words_ref = dict(eng=900000, ger=650000,\n",
    "                     fre=400000, lat=360000, rus=280000,\n",
    "                     jpn=300000, ita=220000, spa=220000)\n",
    "\n",
    "def trim_topwords(row):\n",
    "    if row[0][1:] in top_words_ref:\n",
    "        return top_words_ref[row[0][1:]]\n",
    "    elif row[1] < 100000:\n",
    "        # Ignore langs with practically no words as likely duds, or at the very least\n",
    "        # something BW wouldn't be useful for\n",
    "        return 0\n",
    "    else:\n",
    "        # Other languages: keep greater of 25k or 5% of vocab\n",
    "        mincount = 20000\n",
    "        percentagetrim = int(row[1] * 0.035)\n",
    "        return percentagetrim if percentagetrim > mincount else mincount\n",
    "\n",
    "cutoff_list = tablesizes.reset_index().rename(columns={'index': 'lang', 0: 'count'})\n",
    "cutoff_list['retain_count'] = cutoff_list.apply(trim_topwords, axis=1)\n",
    "print(\"Total tokens (including possible dupes)\", cutoff_list['retain_count'].sum())\n",
    "cutoff_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/eng has 927 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/ger has 195 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/fre has 455 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/lat has 280 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/rus has 146 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/jpn has 280105 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/ita has 398 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/spa has 686 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/chi has 267566 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/und has 3769 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/dut has 540 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/pol has 115 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/cze has 62 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/mul has 1689 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/swe has 188 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/heb has 58 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/kor has 642097 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/scr has 328 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/hun has 161 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/dan has 572 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/ara has 524 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/por has 425 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/gre has 73 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/grc has 178 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/unk has 108 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/scc has 20 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/nor has 377 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/tur has 1529 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/rum has 43 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/ukr has 103 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/fin has 249 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/per has 841 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/bul has 84 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/ind has 1688 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/slo has 50 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/arm has 13782 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/hin has 6635 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/slv has 448 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/tha has 1698 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/lav has 21 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/vie has 1689 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/yid has 21 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/san has 2871 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/ota has 1295 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/cat has 19 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/srp has 130 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/ice has 55 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/cro has 288 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/mar has 2130 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/lit has 9 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/wel has 3 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/urd has 16844 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/sla has 213 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/est has 1 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/pan has 1027 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/geo has 4 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/may has 1 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/aze has 59 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/ben has 5032 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/guj has 635 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/map has 298 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/bos has 1 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/kan has 1402 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/pli has 15 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/jav has 56 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/paa has 1 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/chu has 299 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "/tam has 4163 tokens withs \\u200b in the top 1m\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "problem_dfs = []\n",
    "for i, row in cutoff_list.iterrows():\n",
    "    if row['retain_count'] == 0:\n",
    "        continue\n",
    "    df = pd.read_hdf('/notebooks/data2/final/final-sorted.h5', row['lang'], stop=1000000)\n",
    "    \n",
    "    count = df[(df.index.str.startswith('\\u200b') | df.index.str.endswith(\"\\u200b\"))]\n",
    "    if count.shape[0] != 0:\n",
    "        print(\"%s has %d tokens withs \\\\u200b in the top 1m\" % (row['lang'], count.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n",
      "Final wordlist using top N trim criteria:  (3744070, 1)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "problem_dfs = []\n",
    "for i, row in cutoff_list.iterrows():\n",
    "    if row['retain_count'] == 0:\n",
    "        continue\n",
    "    df = pd.read_hdf('/notebooks/data2/final/final-sorted.h5', row['lang'], stop=row['retain_count'])\n",
    "    # Save Japanese and Chinese chars with \\u200b char\n",
    "    if row['lang'] in ['/jpn', '/chi', '/kor', '/arm', '/urd']:\n",
    "        problems = df[(df.index.str.startswith('\\u200b') | df.index.str.endswith(\"\\u200b\"))]\n",
    "        problem_dfs.append(problems)\n",
    "    dfs.append(df)\n",
    "    \n",
    "asn_probchars = pd.concat(problem_dfs).groupby(level='token').sum().sort_values('count', ascending=False)\n",
    "wordlist = pd.concat(dfs).groupby(level='token').sum().sort_values('count', ascending=False)\n",
    "print(\"Final wordlist using top N trim criteria: \", wordlist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing trim policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n"
     ]
    }
   ],
   "source": [
    "# Grab a 1000 word chunk starting at 'start'\n",
    "start = 9*10**5\n",
    "lang = '/eng'\n",
    "test_tokens = pd.read_hdf('/notebooks/data2/final/final-sorted.h5', lang,\n",
    "                          start=start, stop=start+1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3ul</th>\n",
       "      <td>12451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clcrk</th>\n",
       "      <td>12447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weckesser</th>\n",
       "      <td>12436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>|Feb.</th>\n",
       "      <td>12449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Griiber</th>\n",
       "      <td>12441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unpeople</th>\n",
       "      <td>12442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8-20</th>\n",
       "      <td>12445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.(</th>\n",
       "      <td>12447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palstaves</th>\n",
       "      <td>12439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25,084</th>\n",
       "      <td>12449.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count\n",
       "token             \n",
       "3ul        12451.0\n",
       "Clcrk      12447.0\n",
       "Weckesser  12436.0\n",
       "|Feb.      12449.0\n",
       "Griiber    12441.0\n",
       "unpeople   12442.0\n",
       "(8-20      12445.0\n",
       "18.(       12447.0\n",
       "palstaves  12439.0\n",
       "25,084     12449.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sample, so that you're not biased just to the \n",
    "# same ten tokens at the start of the list. Try re-running this cell\n",
    "test_tokens.sample(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing rules for removing likely junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:10: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "# The re module is broken for hindi and similar characters, need to use regex\n",
    "import regex\n",
    "\n",
    "tokens = wordlist.index\n",
    "hyphenated = tokens.str.contains(r\"-\")\n",
    "#alpha = tokens.str.isalpha() # Faster, but bad for some languages\n",
    "alphaadv = tokens.map(lambda x: not not regex.search(\"^\\\\w+$\", x))\n",
    "number = tokens.str.contains(r\"^(£|$|€)?[\\d.,]+(st|nd|rd|th|s|C|F|c|m|°|¥)?$\")\n",
    "singlequote = tokens.str.contains(r\"[\\'’]\")\n",
    "abbr = tokens.str.contains(r\"^[^\\W\\d]([^\\W\\d]|\\.)+$\")\n",
    "endwithperiod = tokens.str.endswith('.')\n",
    "# This shows up for many asian characters, should be dealt with *before* wordlist is created\n",
    "blankchar = (tokens.str.startswith('\\u200b') | tokens.str.endswith(\"\\u200b\"))\n",
    "tlen = tokens.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /notebooks/data2/final/final-sorted.h5 in read-only mode\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>े</th>\n",
       "      <td>318993347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ा</th>\n",
       "      <td>307119616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ी</th>\n",
       "      <td>254023272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>क</th>\n",
       "      <td>209518823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ं</th>\n",
       "      <td>187441034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "token           \n",
       "े      318993347\n",
       "ा      307119616\n",
       "ी      254023272\n",
       "क      209518823\n",
       "ं      187441034"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf('/notebooks/data2/final/final-sorted.h5', 'hin', stop=1000000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:7: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "tokens = df.index\n",
    "hyphenated = tokens.str.contains(r\"-\")\n",
    "alpha = tokens.str.isalpha() # Faster, but bad for some languages\n",
    "#alphaadv = tokens.map(lambda x: not not regex.search(\"^\\\\w+$\", x))\n",
    "number = tokens.str.contains(r\"^(£|$|€)?[\\d.,]+(st|nd|rd|th|s|C|F|c|m|°|¥)?$\")\n",
    "singlequote = tokens.str.contains(r\"[\\'’]\")\n",
    "abbr = tokens.str.contains(r\"^[^\\W\\d]([^\\W\\d]|\\.)+$\")\n",
    "endwithperiod = tokens.str.endswith('.')\n",
    "# This shows up for many asian characters, should be dealt with *before* wordlist is created\n",
    "blankchar = (tokens.str.startswith('\\u200b') | tokens.str.endswith(\"\\u200b\"))\n",
    "tlen = tokens.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['मे', 'है', 'नही', 'होत', 'जात', 'किय', 'लिए', 'मै', 'हुए', 'कुछ',\n",
       "       'रूप', 'हुआ', 'हैं', 'दिय', 'होन', 'मेर', 'किस', 'कोई', 'साथ',\n",
       "       'जान', 'प्रकार', 'लिय', 'बात', 'वाल', 'हुई', 'क्य', 'फिर', 'द्वार',\n",
       "       'जैस', 'कारण', 'देत', 'जीवन', 'बहुत', 'देख', 'नाम', 'मुझ', 'होग',\n",
       "       'दोनो', 'श्र', 'हू', 'उन्होंन', 'अधिक', 'जिस', 'यहा', 'बाद', 'देन',\n",
       "       'दूसर', 'हिन्द', 'दिन', 'प्राप्त', 'मिल', 'मान', 'हमार', 'उन्हे',\n",
       "       'तुम', 'लेकिन', 'बार', 'था', 'काम', 'राज', 'साहित्य', 'किन्त',\n",
       "       'लिख', 'हो', 'केवल', 'जाय', 'होकर', 'व्यक्त', 'यही', 'पास', 'भाष',\n",
       "       'भारत', 'देश', 'शब्द', 'अन्य', 'चाहिए', 'लोग', 'अनेक', 'दृष्ट',\n",
       "       'मैंन', 'थी', 'जिसक', 'अर्थ', 'कही', 'मिलत', 'काल', 'हाथ', 'चाहत',\n",
       "       'स्थान', 'प्रत', 'सार', 'स्थित', 'परन्त', 'चुक', 'भारतीय', 'भाव',\n",
       "       'प्रयोग', 'वही', 'विचार', 'वहा'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~hyphenated & ~alpha & (tlen >= 2) & ~endwithperiod & ~singlequote & ~number & ~abbr & ~blankchar].index.values[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected junk\n",
    "\n",
    "Top of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&amp;c</th>\n",
       "      <td>85989610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.e</th>\n",
       "      <td>69607602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>है</th>\n",
       "      <td>69227316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)(</th>\n",
       "      <td>66416410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>मे</th>\n",
       "      <td>65622944.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2d</th>\n",
       "      <td>63129683.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.)</th>\n",
       "      <td>60533303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>**</th>\n",
       "      <td>54022272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n°</th>\n",
       "      <td>53972710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8vo</th>\n",
       "      <td>48993113.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "token            \n",
       "&c     85989610.0\n",
       ".e     69607602.0\n",
       "है     69227316.0\n",
       ")(     66416410.0\n",
       "मे     65622944.0\n",
       "2d     63129683.0\n",
       ".)     60533303.0\n",
       "**     54022272.0\n",
       "n°     53972710.0\n",
       "8vo    48993113.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junk = wordlist[~hyphenated & ~alphaadv & (tlen >= 2) & ~endwithperiod & ~singlequote & ~number & ~abbr & ~blankchar]\n",
    "junk.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a random selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>«ll«</th>\n",
       "      <td>29142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de¿</th>\n",
       "      <td>15773.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>小^</th>\n",
       "      <td>2912.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fact—which</th>\n",
       "      <td>14752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,ſcd</th>\n",
       "      <td>5747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l&lt;u3</th>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.Kjobenhavn</th>\n",
       "      <td>5371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.+</th>\n",
       "      <td>907535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>re^ulam</th>\n",
       "      <td>7423.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nach»</th>\n",
       "      <td>236133.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count\n",
       "token                \n",
       "«ll«          29142.0\n",
       "de¿           15773.0\n",
       "小^             2912.0\n",
       "fact—which    14752.0\n",
       ",ſcd           5747.0\n",
       "l<u3            101.0\n",
       ".Kjobenhavn    5371.0\n",
       ".+           907535.0\n",
       "re^ulam        7423.0\n",
       "nach»        236133.0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junk.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPN and CHI fix\n",
    "\n",
    "Any characters in the /jpn and /chi lists that have a non-breaking line space will be added, but with the id of the cleaned version. If there is no cleaned version, add one to the word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_candidate = wordlist[hyphenated | alphaadv | (tlen < 2) | endwithperiod | singlequote | number | abbr | blankchar].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_chars = asn_probchars.reset_index().query('token != \"\\u200b\"')\n",
    "prob_chars['broken'] =prob_chars['token']\n",
    "prob_chars['token'] = prob_chars['broken'].str.replace('\\u200b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Problem characters that are not in the wordlist: add them (as fixed version)\n",
    "to_add = np.setdiff1d(prob_chars['token'].values, final_candidate['token'].values)\n",
    "new_lines = prob_chars[prob_chars['token'].isin(to_add)][['token', 'count']]\n",
    "final = pd.concat([final_candidate, new_lines])\\\n",
    "        .groupby('token', as_index=False).sum()\\\n",
    "        .sort_values('count', ascending=False)\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>broken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123379</th>\n",
       "      <td>3204727</td>\n",
       "      <td>​절감​하고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37404</th>\n",
       "      <td>1636808</td>\n",
       "      <td>​투</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72499</th>\n",
       "      <td>2700091</td>\n",
       "      <td>​一​‥一</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  broken\n",
       "123379  3204727  ​절감​하고\n",
       "37404   1636808      ​투\n",
       "72499   2700091   ​一​‥一"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indices to for the fixed characters. When we encounter the broken words in the dataset, we'll encode then\n",
    "# with the id for the fixed token\n",
    "problemchar_indices = pd.merge(final, prob_chars[['token', 'broken']], on='token')[['index','broken']]\n",
    "problemchar_indices.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final list\n",
    "\n",
    "Also, save /jpn and /chi fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OVERWRITE MODE\n",
    "with pd.HDFStore('/notebooks/data2/final/wordlist.h5', complib='blosc', mode='w', complevel=9) as store:\n",
    "    store.append('/final', final)\n",
    "    store.append('/fixes', problemchar_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test against dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils\n",
    "\n",
    "# Two copies of the same dictionary, Laird and Lee's Webster's. They capitalize their words, so\n",
    "# I'm looking for capital words that occur in both.\n",
    "dicts = ['loc.ark:/13960/t84j1sb5j', 'loc.ark:/13960/t3xs70k06']\n",
    "paths = ['/notebooks/features/' + utils.id_to_rsync(volid) for volid in dicts]\n",
    "fr = FeatureReader(paths)\n",
    "tokenlist = []\n",
    "for vol in fr.volumes():\n",
    "    tokenlist += vol.tokens()\n",
    "\n",
    "tokens = pd.Series(tokenlist)\n",
    "# Grab capitalized letters\n",
    "dictionary_words = tokens[tokens.str.contains(r\"^[A-Z][A-Z\\-]*$\")].value_counts()\n",
    "shortlist = dictionary_words[dictionary_words > 1].index.str.lower().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_final_lower = final['token'].str.lower().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extradictwords = np.setdiff1d(shortlist, unique_final_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4488           ostracizing\n",
       "6507              vaticide\n",
       "5673             spikefish\n",
       "5296        salubriousness\n",
       "5484            shellpboof\n",
       "3131            faith-cure\n",
       "5981         tassel-flower\n",
       "4336               nautili\n",
       "3820    irreconcilableness\n",
       "359             appeasable\n",
       "2326          dermatophone\n",
       "3607               hushaby\n",
       "318             antitheism\n",
       "3621             hygienics\n",
       "441               ascidium\n",
       "5796         stormy-petrel\n",
       "1965          crossed-wire\n",
       "4636            peccancies\n",
       "2855               dunfish\n",
       "4539               ozonous\n",
       "5959              tanistry\n",
       "2538        discontinuable\n",
       "3821         irreligiously\n",
       "1395           cheese-cake\n",
       "5985              tatizing\n",
       "6146       through-lighted\n",
       "6545            vesiculous\n",
       "3922               keratol\n",
       "5495         ship-chandler\n",
       "6330               tu-whit\n",
       "905               blazoner\n",
       "1507            clangoring\n",
       "178            alleviatory\n",
       "4997               quizzer\n",
       "3372          germiculture\n",
       "240              analepsis\n",
       "6317           tropic-bird\n",
       "6353            turkey-hen\n",
       "6594             volucrine\n",
       "2429         dialogistical\n",
       "1015         bounteousness\n",
       "6479            uxoriously\n",
       "4699                 pewit\n",
       "6598           vortiginous\n",
       "2743          double-eagle\n",
       "1855                cosher\n",
       "3884            joy-riding\n",
       "239          anagrammatize\n",
       "1643          commemorator\n",
       "758                bee-gum\n",
       "dtype: object"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(extradictwords).sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44169, 6814, 0.8457289048880436)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortlist.shape[0], extradictwords.shape[0], 1-extradictwords.shape[0]/shortlist.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk filter testing, here be dragons\n",
    "\n",
    "Keeping this here as an example of how I tested various matching criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng_2m = dd.read_hdf('/notebooks/data2/final/final-sorted.h5', '/eng', stop=2000000).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:9: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:10: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:11: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "tokens = eng_2m.index\n",
    "alpha = tokens.str.isalpha()\n",
    "digit = tokens.str.isdigit()\n",
    "tlen = tokens.str.len()\n",
    "endwithperiod = tokens.str.endswith('.')\n",
    "quotes = (tokens.str.startswith('\"') | tokens.str.endswith('\"')) # | tokens.str.startswith('\\'') | tokens.str.endswith('\\''))\n",
    "endash = (tokens.str.startswith('—') | tokens.str.endswith('—'))\n",
    "punccount = tokens.str.count('[\\W]')\n",
    "repeating = tokens.str.contains(r\"(([\\w\\W])\\2{3,})\")\n",
    "repeatingdigit = tokens.str.contains(r\"((\\d)\\2{3,})\")\n",
    "abbr = tokens.str.contains(r\"^[^\\W\\d]([^\\W\\d]|\\.)+$\")\n",
    "singlequote = tokens.str.contains(r\"\\'\")\n",
    "hyphenated = tokens.str.contains(r\"-\")\n",
    "number = tokens.str.contains(r\"^(£|$|€)?[\\d.,]+(st|nd|rd|th|s|C|F|c|m|°|¥)?$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>2.447435e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n't</th>\n",
       "      <td>2.651758e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'d</th>\n",
       "      <td>1.386013e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--</th>\n",
       "      <td>8.163921e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&amp;c</th>\n",
       "      <td>6.846841e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.e</th>\n",
       "      <td>6.169432e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'ll</th>\n",
       "      <td>6.003169e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>5.869751e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)(</th>\n",
       "      <td>5.712112e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.1</th>\n",
       "      <td>5.353481e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "token              \n",
       "'s     2.447435e+09\n",
       "n't    2.651758e+08\n",
       "'d     1.386013e+08\n",
       "--     8.163921e+07\n",
       "&c     6.846841e+07\n",
       ".e     6.169432e+07\n",
       "'ll    6.003169e+07\n",
       "''     5.869751e+07\n",
       ")(     5.712112e+07\n",
       ".1     5.353481e+07"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_2m[~alpha & (tlen > 1) & ~endwithperiod & (punccount >= 1)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4047440,) (2000000,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-659-babfea1a170b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meng_2m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mhyphenated\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtlen\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mendwithperiod\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0msinglequote\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mnumber\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mabbr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4047440,) (2000000,) "
     ]
    }
   ],
   "source": [
    "a = eng_2m[~hyphenated & ~alpha & (tlen >= 2) & ~endwithperiod & ~singlequote & ~number & ~abbr]\n",
    "print(\"\\t\".join(a[:300].index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N.Y</th>\n",
       "      <td>19090114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e.g</th>\n",
       "      <td>9622439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N.J</th>\n",
       "      <td>5433056.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___</th>\n",
       "      <td>4738726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a.d</th>\n",
       "      <td>4416815.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N.C</th>\n",
       "      <td>4102392.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.Ct</th>\n",
       "      <td>3522386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m.p</th>\n",
       "      <td>2893462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n.d</th>\n",
       "      <td>2620175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F.Supp</th>\n",
       "      <td>2236074.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h.p</th>\n",
       "      <td>1890368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M.D</th>\n",
       "      <td>1876216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f.o.b</th>\n",
       "      <td>1830278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.p</th>\n",
       "      <td>1511469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I.e</th>\n",
       "      <td>1500893.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U.S</th>\n",
       "      <td>1490175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L.Ed</th>\n",
       "      <td>1447475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v.t</th>\n",
       "      <td>1442048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D.C</th>\n",
       "      <td>1339182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B.Sc</th>\n",
       "      <td>1266500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no.l</th>\n",
       "      <td>1216545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.m</th>\n",
       "      <td>1015733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p.s</th>\n",
       "      <td>988511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r.p.m</th>\n",
       "      <td>985256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v.p</th>\n",
       "      <td>966616.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m.p.h</th>\n",
       "      <td>941082.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p.a</th>\n",
       "      <td>939271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_of</th>\n",
       "      <td>930984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p.c</th>\n",
       "      <td>924845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M.A</th>\n",
       "      <td>912447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E.y</th>\n",
       "      <td>4336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_gi</th>\n",
       "      <td>4335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I.SS</th>\n",
       "      <td>4335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD.ERROR</th>\n",
       "      <td>4334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b.ad</th>\n",
       "      <td>4334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G.Yu</th>\n",
       "      <td>4334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montg.Co</th>\n",
       "      <td>4333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A.E.IL</th>\n",
       "      <td>4331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J.White</th>\n",
       "      <td>4330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O.NO</th>\n",
       "      <td>4330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f.i.c</th>\n",
       "      <td>4329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trev.Higd</th>\n",
       "      <td>4329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E.Jackson</th>\n",
       "      <td>4329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr.y</th>\n",
       "      <td>4328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g.Au</th>\n",
       "      <td>4327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_Am</th>\n",
       "      <td>4327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oo.ooo</th>\n",
       "      <td>4327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>т.в</th>\n",
       "      <td>4326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes_</th>\n",
       "      <td>4326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ann.St</th>\n",
       "      <td>4326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www.nber</th>\n",
       "      <td>4325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_Borough</th>\n",
       "      <td>4323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of_an</th>\n",
       "      <td>4323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_Then</th>\n",
       "      <td>4322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INST.CR</th>\n",
       "      <td>4322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J.ii</th>\n",
       "      <td>4322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T.fl</th>\n",
       "      <td>4321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F.y</th>\n",
       "      <td>4321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E.Mey</th>\n",
       "      <td>4320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I.St</th>\n",
       "      <td>4320.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7455 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                count\n",
       "token                \n",
       "N.Y        19090114.0\n",
       "e.g         9622439.0\n",
       "N.J         5433056.0\n",
       "___         4738726.0\n",
       "a.d         4416815.0\n",
       "N.C         4102392.0\n",
       "S.Ct        3522386.0\n",
       "m.p         2893462.0\n",
       "n.d         2620175.0\n",
       "F.Supp      2236074.0\n",
       "h.p         1890368.0\n",
       "M.D         1876216.0\n",
       "f.o.b       1830278.0\n",
       "s.p         1511469.0\n",
       "I.e         1500893.0\n",
       "U.S         1490175.0\n",
       "L.Ed        1447475.0\n",
       "v.t         1442048.0\n",
       "D.C         1339182.0\n",
       "B.Sc        1266500.0\n",
       "no.l        1216545.0\n",
       "s.m         1015733.0\n",
       "p.s          988511.0\n",
       "r.p.m        985256.0\n",
       "v.p          966616.0\n",
       "m.p.h        941082.0\n",
       "p.a          939271.0\n",
       "_of          930984.0\n",
       "p.c          924845.0\n",
       "M.A          912447.0\n",
       "...               ...\n",
       "E.y            4336.0\n",
       "_gi            4335.0\n",
       "I.SS           4335.0\n",
       "STD.ERROR      4334.0\n",
       "b.ad           4334.0\n",
       "G.Yu           4334.0\n",
       "Montg.Co       4333.0\n",
       "A.E.IL         4331.0\n",
       "J.White        4330.0\n",
       "O.NO           4330.0\n",
       "f.i.c          4329.0\n",
       "Trev.Higd      4329.0\n",
       "E.Jackson      4329.0\n",
       "mr.y           4328.0\n",
       "g.Au           4327.0\n",
       "_Am            4327.0\n",
       "oo.ooo         4327.0\n",
       "т.в            4326.0\n",
       "Yes_           4326.0\n",
       "Ann.St         4326.0\n",
       "www.nber       4325.0\n",
       "_Borough       4323.0\n",
       "of_an          4323.0\n",
       "_Then          4322.0\n",
       "INST.CR        4322.0\n",
       "J.ii           4322.0\n",
       "T.fl           4321.0\n",
       "F.y            4321.0\n",
       "E.Mey          4320.0\n",
       "I.St           4320.0\n",
       "\n",
       "[7455 rows x 1 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_2m[~alpha & abbr & ~endwithperiod & (tlen > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "abbr = tokens.str.contains(r\"^[^\\W\\d]([^\\W\\d]|\\.)+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.176438"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_2m[~hyphenated & ~abbr & ~alpha & (tlen >= 2) & ~endwithperiod & ~singlequote & ~number].shape[0] /2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000000000000</th>\n",
       "      <td>266011.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111111111</th>\n",
       "      <td>156061.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000</th>\n",
       "      <td>146507.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111111</th>\n",
       "      <td>132444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000</th>\n",
       "      <td>101206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000</th>\n",
       "      <td>96323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111111111111</th>\n",
       "      <td>90377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000000000000</th>\n",
       "      <td>67591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111111111111</th>\n",
       "      <td>60583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000</th>\n",
       "      <td>57728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111111111</th>\n",
       "      <td>49005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000000</th>\n",
       "      <td>41855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000000000000000000000</th>\n",
       "      <td>39671.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000000000000000</th>\n",
       "      <td>39057.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222222222222</th>\n",
       "      <td>36633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111111111111111111</th>\n",
       "      <td>34584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000</th>\n",
       "      <td>34428.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000</th>\n",
       "      <td>28726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111111111111111</th>\n",
       "      <td>26850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888888888888</th>\n",
       "      <td>25982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88888888888</th>\n",
       "      <td>25499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111111111111111111111111111111111111111111111111</th>\n",
       "      <td>25269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000000000000000000000000000000000000</th>\n",
       "      <td>22719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000000000000000000</th>\n",
       "      <td>22073.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000000</th>\n",
       "      <td>21611.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111111111111111</th>\n",
       "      <td>20561.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111111111111</th>\n",
       "      <td>20462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111111111111111111</th>\n",
       "      <td>20337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000000000</th>\n",
       "      <td>18920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888888888888</th>\n",
       "      <td>16994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444444444444</th>\n",
       "      <td>6125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111111111111111111111111</th>\n",
       "      <td>6004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999999999</th>\n",
       "      <td>5999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44444444444</th>\n",
       "      <td>5990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000000000000000000</th>\n",
       "      <td>5974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888888888888888888</th>\n",
       "      <td>5958.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111111111111111111111111</th>\n",
       "      <td>5833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111111111111111111111111111</th>\n",
       "      <td>5586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55555555555</th>\n",
       "      <td>5548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111111111111111111111111111111</th>\n",
       "      <td>5543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888888888888888888</th>\n",
       "      <td>5421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111011111</th>\n",
       "      <td>5407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999999999</th>\n",
       "      <td>5378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000000000000000000000</th>\n",
       "      <td>5340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88888888888888888888</th>\n",
       "      <td>5311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111111111111111111111111111</th>\n",
       "      <td>5077.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111111110</th>\n",
       "      <td>4992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101111111</th>\n",
       "      <td>4916.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111101111</th>\n",
       "      <td>4913.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333333333333</th>\n",
       "      <td>4884.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222222222222222</th>\n",
       "      <td>4858.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77777777777777777777777777777777777777777777777777</th>\n",
       "      <td>4847.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101111111111</th>\n",
       "      <td>4845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999999999999999999</th>\n",
       "      <td>4764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000000000</th>\n",
       "      <td>4739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777777777777777</th>\n",
       "      <td>4590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110111111111</th>\n",
       "      <td>4380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888888888888888888888</th>\n",
       "      <td>4366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111111111111111111111111111111</th>\n",
       "      <td>4352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111111011111</th>\n",
       "      <td>4331.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       count\n",
       "token                                                       \n",
       "000000000000                                        266011.0\n",
       "11111111111                                         156061.0\n",
       "00000000000                                         146507.0\n",
       "111111111111                                        132444.0\n",
       "00000000000000                                      101206.0\n",
       "0000000000000                                        96323.0\n",
       "1111111111111                                        90377.0\n",
       "000000000000000                                      67591.0\n",
       "11111111111111                                       60583.0\n",
       "0000000000000000                                     57728.0\n",
       "111111111111111                                      49005.0\n",
       "00000000000000000000                                 41855.0\n",
       "000000000000000000000000                             39671.0\n",
       "000000000000000000                                   39057.0\n",
       "222222222222                                         36633.0\n",
       "111111111111111111111111                             34584.0\n",
       "00000000000000000                                    34428.0\n",
       "0000000000000000000                                  28726.0\n",
       "1111111111111111                                     26850.0\n",
       "888888888888                                         25982.0\n",
       "88888888888                                          25499.0\n",
       "11111111111111111111111111111111111111111111111111   25269.0\n",
       "00000000000000000000000000000000000000000000000000   22719.0\n",
       "000000000000000000000                                22073.0\n",
       "0000000000000000000000                               21611.0\n",
       "11111111111111111                                    20561.0\n",
       "111111111111111111                                   20462.0\n",
       "11111111111111111111                                 20337.0\n",
       "00000000000000000000000                              18920.0\n",
       "8888888888888                                        16994.0\n",
       "...                                                      ...\n",
       "444444444444                                          6125.0\n",
       "11111111111111111111111111                            6004.0\n",
       "99999999999                                           5999.0\n",
       "44444444444                                           5990.0\n",
       "0000000000000000000000000000000000                    5974.0\n",
       "888888888888888888                                    5958.0\n",
       "111111111111111111111111111111                        5833.0\n",
       "1111111111111111111111111111                          5586.0\n",
       "55555555555                                           5548.0\n",
       "1111111111111111111111111111111                       5543.0\n",
       "8888888888888888888                                   5421.0\n",
       "11111011111                                           5407.0\n",
       "999999999999                                          5378.0\n",
       "0000000000000000000000000000000000000                 5340.0\n",
       "88888888888888888888                                  5311.0\n",
       "11111111111111111111111111111                         5077.0\n",
       "111111111110                                          4992.0\n",
       "11101111111                                           4916.0\n",
       "11111101111                                           4913.0\n",
       "3333333333333                                         4884.0\n",
       "222222222222222                                       4858.0\n",
       "77777777777777777777777777777777777777777777777777    4847.0\n",
       "1101111111111                                         4845.0\n",
       "99999999999999999999                                  4764.0\n",
       "10000000000                                           4739.0\n",
       "777777777777777                                       4590.0\n",
       "1110111111111                                         4380.0\n",
       "888888888888888888888                                 4366.0\n",
       "11111111111111111111111111111111                      4352.0\n",
       "111111011111                                          4331.0\n",
       "\n",
       "[109 rows x 1 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Longest digit-only values\n",
    "eng_2m[digit & (tlen > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>13254850187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>34517225417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>49027500884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that</td>\n",
       "      <td>7243618116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>which</td>\n",
       "      <td>3549593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>should</td>\n",
       "      <td>864776997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>between</td>\n",
       "      <td>612977820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>American</td>\n",
       "      <td>304614658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>following</td>\n",
       "      <td>335142138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>University</td>\n",
       "      <td>241811028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>information</td>\n",
       "      <td>217085205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>construction</td>\n",
       "      <td>128267157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>consideration</td>\n",
       "      <td>102522251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>administration</td>\n",
       "      <td>69023302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>characteristics</td>\n",
       "      <td>48779362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>responsibilities</td>\n",
       "      <td>14456755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>misrepresentation</td>\n",
       "      <td>2319613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>telecommunications</td>\n",
       "      <td>2690164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hyperparathyroidism</td>\n",
       "      <td>358987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>institutionalization</td>\n",
       "      <td>501022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kernforschungszentrum</td>\n",
       "      <td>115585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>spectrophotometrically</td>\n",
       "      <td>200554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>electroencephalographic</td>\n",
       "      <td>107924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phosphatidylethanolamine</td>\n",
       "      <td>120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phosphoribosyltransferase</td>\n",
       "      <td>49492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ethylenediaminetetraacetic</td>\n",
       "      <td>42004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ethylenediaminetetraacetate</td>\n",
       "      <td>27375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>13019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Wasserstoffionenkonzentration</td>\n",
       "      <td>13303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>dipalmitoylphosphatidylcholine</td>\n",
       "      <td>16485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>9599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>10054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>8462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>6763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>JFMAMJJASONDJFMAMJJASONDJFMAMJJASOND</td>\n",
       "      <td>11566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>5459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>4771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>5358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII</td>\n",
       "      <td>4767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          token       count\n",
       "chars                                                      \n",
       "1                                             a 13254850187\n",
       "2                                            of 34517225417\n",
       "3                                           the 49027500884\n",
       "4                                          that  7243618116\n",
       "5                                         which  3549593200\n",
       "6                                        should   864776997\n",
       "7                                       between   612977820\n",
       "8                                      American   304614658\n",
       "9                                     following   335142138\n",
       "10                                   University   241811028\n",
       "11                                  information   217085205\n",
       "12                                 construction   128267157\n",
       "13                                consideration   102522251\n",
       "14                               administration    69023302\n",
       "15                              characteristics    48779362\n",
       "16                             responsibilities    14456755\n",
       "17                            misrepresentation     2319613\n",
       "18                           telecommunications     2690164\n",
       "19                          hyperparathyroidism      358987\n",
       "20                         institutionalization      501022\n",
       "21                        Kernforschungszentrum      115585\n",
       "22                       spectrophotometrically      200554\n",
       "23                      electroencephalographic      107924\n",
       "24                     phosphatidylethanolamine      120900\n",
       "25                    phosphoribosyltransferase       49492\n",
       "26                   ethylenediaminetetraacetic       42004\n",
       "27                  ethylenediaminetetraacetate       27375\n",
       "28                 IIIIIIIIIIIIIIIIIIIIIIIIIIII       13019\n",
       "29                Wasserstoffionenkonzentration       13303\n",
       "30               dipalmitoylphosphatidylcholine       16485\n",
       "31              IIIIIIIIIIIIIIIIIIIIIIIIIIIIIII        9599\n",
       "32             IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII       10054\n",
       "33            IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII        8462\n",
       "34           IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII        7999\n",
       "35          IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII        6763\n",
       "36         JFMAMJJASONDJFMAMJJASONDJFMAMJJASOND       11566\n",
       "37        IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII        5459\n",
       "38       IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII        4771\n",
       "39      IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII        5358\n",
       "40     IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII        4767"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Long words\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "longwords = eng_2m[alpha].reset_index().copy()\n",
    "longwords['chars'] = longwords['token'].str.len()\n",
    "longwords.head(1)\n",
    "longwords.groupby('chars').apply(lambda x: x.sort_values('count').iloc[-1] )[['token', 'count']][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912474, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_nonalpha = eng_2m[~eng_2m.index.str.isalpha()]\n",
    "eng_nonalpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51845610, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_nonalphanumeric = eng_nonalpha[~eng_nonalpha.index.str.isdigit()]\n",
    "eng_nonalphanumeric.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
