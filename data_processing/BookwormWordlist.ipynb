{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Step 1: Processing Feature Files for Bookworm\n",
    "\n",
    "This notebook runs through Extracted Features files, saving:\n",
    "\n",
    "1. Global token counts (by language) toward the eventual Bookworm Wordlist. \n",
    "    These aren't all folded here: rather, they are folded by batch and saved to and HDF5 Store.\n",
    "    Later, they'll all be folded into one big list.\n",
    "\n",
    "2. \"Raw\" unigram counts per book. These will eventually be trimmed to only the BW vocabulary and\n",
    "    labelled by an id. This information first needs the wordlist that #1 above will create, but\n",
    "    since we're already opening the EF files, might as well do some processing and save this\n",
    "    intermediate state to a fast IO format (HDF5 store, again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook # Progress bars!\n",
    "from ipyparallel import Client\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before attaching to ipyparallel engines, they need to be started with \n",
    "\n",
    "```bash\n",
    "    ipcluster start -n NUM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = Client()\n",
    "dview = rc[:]\n",
    "v = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize logging. There's no nice way to pass logs between engines, so just give each one its own log.\n",
    "\n",
    "The timestamp format is designed for easy sort, so you can track all logs with \n",
    "\n",
    "```bash\n",
    "watch \"tail -q -n 100 logs/* | sort\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_log(name=False):\n",
    "    import logging, os\n",
    "    if not name:\n",
    "        name = os.getpid()\n",
    "    handler = logging.FileHandler(\"/notebooks/data/logs/bw-%s.log\" % name, 'a')\n",
    "    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s', \"%m/%d-%H:%M:%S\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logging.info(\"Log initialized\")\n",
    "\n",
    "dview.push(dict(init_log=init_log))\n",
    "%px init_log()\n",
    "init_log(\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load paths to feature files. This notebook maintains a list of successully processed ids, so there are some functions that help us cross reference all volumes with done volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts 4805430\n"
     ]
    }
   ],
   "source": [
    "with open(\"/notebooks/features/listing/pd-file-listing.txt\", \"r\") as f:\n",
    "    paths = [\"/notebooks/features/\"+path.strip() for path in f.readlines()][1:]\n",
    "    print(\"Number of texts\", len(paths))\n",
    "\n",
    "def get_processed():\n",
    "    ''' Get already processed files. Wrapped in func for easy refresh'''\n",
    "    try:\n",
    "        with open(\"successful-counts.txt\", \"r\") as f:\n",
    "            paths = f.read().strip().split(\"\\n\")\n",
    "        paths = [\"/notebooks/features/\"+utils.id_to_rsync(path) for path in paths]\n",
    "        return np.array(paths)\n",
    "    except:\n",
    "        return np.array([])\n",
    "\n",
    "path_to_id = lambda x: x.replace(\".json.bz2\", \"\").split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_count` is the function that does the processing of the volume. To improve performance, however, the subprocesses run larger volumes in larger batches with `get_doc_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">eng</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">ufl2.uf00100909_00001</th>\n",
       "      <th>!</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"</th>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"\"2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      count\n",
       "language id                    token       \n",
       "eng      ufl2.uf00100909_00001 !          1\n",
       "                               \"        397\n",
       "                               \"\"2        1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trim_token(t, max=50):\n",
    "    ''' Trim unicode string to max number of bytes'''\n",
    "    if len(t.encode('utf-8')) > max:\n",
    "        while len(t.encode('utf-8')) > max:\n",
    "            t = t[:-1]\n",
    "    return t\n",
    "\n",
    "def get_count(path, store=False):\n",
    "    ''' Get tokencount information from a single doc, by path'''\n",
    "    from htrc_features import FeatureReader    \n",
    "    max_char = 50\n",
    "    vol = FeatureReader(path).first()\n",
    "    tl = vol.tokenlist(pages=False, pos=False)\n",
    "    if tl.empty:\n",
    "        return tl\n",
    "    else:\n",
    "        tl = tl.reset_index('section')[['count']]\n",
    "    tl.index = [trim_token(t, max_char) for t in tl.index.values]\n",
    "    tl.index.names=['token']\n",
    "    tl['id'] = vol.id\n",
    "    tl['language'] = vol.language\n",
    "    tl = tl.reset_index('token').set_index(['language', 'id', 'token']).sort_index()\n",
    "    return tl\n",
    "\n",
    "# Send to Engines\n",
    "dview.push(dict(trim_token=trim_token, get_count=get_count))\n",
    "\n",
    "# Example\n",
    "get_count(paths[0]).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_counts(paths, mincount=2, max_str_bytes = 50):\n",
    "    '''\n",
    "    This method lets you process multiple paths at a time on a single engine.\n",
    "    This means the engine can collect enough texts to do a simple filter (i.e. >X counts in Y texts)\n",
    "    and can save to it's own store.\n",
    "    '''\n",
    "    import logging\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    fname = '/notebooks/data/stores/bw_counts_%s.h5' % os.getpid()\n",
    "    success_log = []\n",
    "    logging.info(\"Starting %d volume batch on PID=%s\" % (len(paths), os.getpid()))\n",
    "    with pd.HDFStore(fname, mode=\"a\", complevel=9, complib='blosc') as store:\n",
    "        tl_collector = []\n",
    "        for path in paths:\n",
    "            try:\n",
    "                tl = get_count(path, store=store)\n",
    "                if tl.empty:\n",
    "                    continue\n",
    "                tl_collector.append(tl)\n",
    "            except:\n",
    "                logging.exception(\"Unable to get count for path %s\" % path)\n",
    "                continue\n",
    "            success_log.append(path)\n",
    "\n",
    "        # Save a DF combining all the counts from this batch\n",
    "        try:\n",
    "            logging.info(\"Merging and Saving texts for %d paths starting with %s\" % (len(paths), paths[0]))\n",
    "            combineddf = pd.concat(tl_collector)\n",
    "            \n",
    "            # Save tf(doc) with volid but no lang\n",
    "            # For efficient HDF5 storage, enforcing a 50 byte token limit. Can't use\n",
    "            # DataFrame.str.slice(stop=50) though, because really we care about bytes and \n",
    "            # some unicode chars are multiple codepoints.\n",
    "            # volids are capped at 25chars (the longest PD vol id)\n",
    "            store.append('tf/docs',\n",
    "                         combineddf.reset_index('language')[['count']],\n",
    "                         min_itemsize = {'id': 25, 'token':max_str_bytes})\n",
    "            \n",
    "            ### Save tf(corpus)\n",
    "            df = combineddf.groupby(level=['language', 'token'])[['count']]\\\n",
    "                           .sum().sort_index()\n",
    "            # Filtering this way (by corpus total, not language total) is too slow:\n",
    "            #if mincount:\n",
    "            #    df = df.groupby(level='token')[['count']].filter(lambda x: x.sum()>=mincount)\n",
    "            # Because we can't feasibly filter on total count and have to do so by lang x token, it\n",
    "            # might unfairly punish sparse languages. My workaround is to only even trim English by\n",
    "            # mincount: any bias this would have would be in the bottom of the wordlist anyway.\n",
    "            if mincount:\n",
    "                df = df[(df.index.get_level_values(0) != 'eng') | (df['count']>2)]\n",
    "            store.append('tf/corpus', df, min_itemsize = {'token': max_str_bytes})\n",
    "            tl_collector = dict()\n",
    "            return success_log\n",
    "        except:\n",
    "            logging.exception(\"Saving error for %d paths starting with %s\" % (len(paths), paths[0]))\n",
    "            return []\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting parallel job\n",
      "INFO:root:Done processing batch 1, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0n/s1/nr/92/ark+=13960=t0ns1nr92/aeu.ark+=13960=t0ns1nr92.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0q/r5/5f/0p/ark+=13960=t0qr55f0p/aeu.ark+=13960=t0qr55f0p.json.bz2\n",
      "INFO:root:Done processing batch 2, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0r/r2/vq/0q/ark+=13960=t0rr2vq0q/aeu.ark+=13960=t0rr2vq0q.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0t/q6/g5/0s/ark+=13960=t0tq6g50s/aeu.ark+=13960=t0tq6g50s.json.bz2\n",
      "INFO:root:Done processing batch 3, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0g/t6/pr/0t/ark+=13960=t0gt6pr0t/aeu.ark+=13960=t0gt6pr0t.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0j/t0/84/42/ark+=13960=t0jt08442/aeu.ark+=13960=t0jt08442.json.bz2\n",
      "INFO:root:Done processing batch 4, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0c/v5/8m/60/ark+=13960=t0cv58m60/aeu.ark+=13960=t0cv58m60.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0d/v3/bh/04/ark+=13960=t0dv3bh04/aeu.ark+=13960=t0dv3bh04.json.bz2\n",
      "INFO:root:Done processing batch 5, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/00/z8/11/9p/ark+=13960=t00z8119p/aeu.ark+=13960=t00z8119p.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/01/z6/0p/2z/ark+=13960=t01z60p2z/aeu.ark+=13960=t01z60p2z.json.bz2\n",
      "INFO:root:Done processing batch 6, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0t/q6/g6/07/ark+=13960=t0tq6g607/aeu.ark+=13960=t0tq6g607.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0v/q4/2v/1t/ark+=13960=t0vq42v1t/aeu.ark+=13960=t0vq42v1t.json.bz2\n",
      "INFO:root:Done processing batch 7, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/09/w1/r7/3m/ark+=13960=t09w1r73m/aeu.ark+=13960=t09w1r73m.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0c/v5/8h/17/ark+=13960=t0cv58h17/aeu.ark+=13960=t0cv58h17.json.bz2\n",
      "INFO:root:Done processing batch 8, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/06/x0/m5/8h/ark+=13960=t06x0m58h/aeu.ark+=13960=t06x0m58h.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/08/w4/19/9j/ark+=13960=t08w4199j/aeu.ark+=13960=t08w4199j.json.bz2\n",
      "INFO:root:Done processing batch 9, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/03/x9/3m/17/ark+=13960=t03x93m17/aeu.ark+=13960=t03x93m17.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/05/x2/sc/2t/ark+=13960=t05x2sc2t/aeu.ark+=13960=t05x2sc2t.json.bz2\n",
      "INFO:root:Done processing batch 10, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0x/p7/ph/8k/ark+=13960=t0xp7ph8k/aeu.ark+=13960=t0xp7ph8k.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0z/p5/b6/9f/ark+=13960=t0zp5b69f/aeu.ark+=13960=t0zp5b69f.json.bz2\n",
      "INFO:root:Done processing batch 11, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0j/t0/89/5s/ark+=13960=t0jt0895s/aeu.ark+=13960=t0jt0895s.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0k/s8/0k/52/ark+=13960=t0ks80k52/aeu.ark+=13960=t0ks80k52.json.bz2\n",
      "INFO:root:Done processing batch 12, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0v/q4/2v/67/ark+=13960=t0vq42v67/aeu.ark+=13960=t0vq42v67.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0x/p7/ph/4n/ark+=13960=t0xp7ph4n/aeu.ark+=13960=t0xp7ph4n.json.bz2\n",
      "INFO:root:Done processing batch 13, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/01/z6/0t/75/ark+=13960=t01z60t75/aeu.ark+=13960=t01z60t75.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/03/x9/3j/2t/ark+=13960=t03x93j2t/aeu.ark+=13960=t03x93j2t.json.bz2\n",
      "INFO:root:Done processing batch 14, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/08/w4/1d/7x/ark+=13960=t08w41d7x/aeu.ark+=13960=t08w41d7x.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/09/w1/r4/57/ark+=13960=t09w1r457/aeu.ark+=13960=t09w1r457.json.bz2\n",
      "INFO:root:Done processing batch 15, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/05/x2/sd/1s/ark+=13960=t05x2sd1s/aeu.ark+=13960=t05x2sd1s.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/06/x0/k7/3k/ark+=13960=t06x0k73k/aeu.ark+=13960=t06x0k73k.json.bz2\n",
      "INFO:root:Done processing batch 16, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0z/p5/b7/8d/ark+=13960=t0zp5b78d/aeu.ark+=13960=t0zp5b78d.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/11/n8/w1/24/ark+=13960=t11n8w124/aeu.ark+=13960=t11n8w124.json.bz2\n",
      "INFO:root:Done processing batch 17, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0d/v3/bn/1d/ark+=13960=t0dv3bn1d/aeu.ark+=13960=t0dv3bn1d.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0g/t6/pp/79/ark+=13960=t0gt6pp79/aeu.ark+=13960=t0gt6pp79.json.bz2\n",
      "INFO:root:Done processing batch 18, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0q/r5/5g/23/ark+=13960=t0qr55g23/aeu.ark+=13960=t0qr55g23.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0r/r2/vh/4z/ark+=13960=t0rr2vh4z/aeu.ark+=13960=t0rr2vh4z.json.bz2\n",
      "INFO:root:Done processing batch 19, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0k/s8/0r/2v/ark+=13960=t0ks80r2v/aeu.ark+=13960=t0ks80r2v.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/0n/s1/n7/5d/ark+=13960=t0ns1n75d/aeu.ark+=13960=t0ns1n75d.json.bz2\n",
      "INFO:root:Done processing batch 20, from /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/00/00/8f/0x/ark+=13960=t00008f0x/aeu.ark+=13960=t00008f0x.json.bz2 to /notebooks/features/aeu/pairtree_root/ar/k+/=1/39/60/=t/00/z8/0v/34/ark+=13960=t00z80v34/aeu.ark+=13960=t00z80v34.json.bz2\n",
      "INFO:root:Done\n",
      "INFO:root:397.31851267814636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Split paths into N-sized chunks, so engines can iterate on multiple texts at once\n",
    "chunk_size = 400\n",
    "i = 0\n",
    "# To avoid human error resulting in duplicated or missed files, simply trim the \n",
    "# Path list when this cell is run\n",
    "remaining_paths = np.setdiff1d(paths, get_processed())\n",
    "chunked_paths = [remaining_paths[i:i+chunk_size] for i in range(0, len(remaining_paths), chunk_size)]\n",
    "n = 20\n",
    "start = 0\n",
    "\n",
    "starttime = time.time()\n",
    "logging.info(\"Starting parallel job\")\n",
    "parallel_job = v.map(get_doc_counts, chunked_paths[start:start+n], ordered=False)\n",
    "\n",
    "for result in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    i += 1\n",
    "    if result:\n",
    "        with open(\"/notebooks/data/successful-counts.txt\", \"a+\") as f:\n",
    "            ids = [path_to_id(path) for path in result]\n",
    "            f.write(\"\\n\".join(ids)+\"\\n\")\n",
    "        logging.info(\"Done processing batch %d, from %s to %s\" % (i, result[0], result[-1]))\n",
    "    else:\n",
    "        logging.error(\"Problem with result in batch %d\" % i)\n",
    "\n",
    "logging.info(\"Done\")\n",
    "logging.info(time.time()-starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_75.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->880777,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->3295309,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_81.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->560487,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4016176,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_92.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->734163,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4848439,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_159.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->612326,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4104608,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_71.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->876067,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4302540,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_83.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->600440,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->3645897,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_57.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->811898,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->3696084,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_101.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->804245,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->3638911,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_175.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->709285,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4000554,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_73.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->1146557,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4744803,ncols->3,indexers->[index],dc->[token,id])      \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_79.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->808812,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4733874,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_103.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->770051,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->3830765,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_157.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->771396,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4535922,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_77.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->1182244,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4321652,ncols->3,indexers->[index],dc->[token,id])      \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_139.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->862072,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->3784348,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_121.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->798223,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4171748,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_130.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->717509,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4360332,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_56.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->928963,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4339145,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_112.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->985298,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->4862130,ncols->3,indexers->[index],dc->[token,id])     \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /notebooks/data/stores/bw_counts_148.h5\n",
      "/tf/corpus            frame_table  (typ->appendable_multi,nrows->594221,ncols->3,indexers->[index],dc->[token,language])\n",
      "/tf/docs              frame_table  (typ->appendable_multi,nrows->3426438,ncols->3,indexers->[index],dc->[token,id])     \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "stores = glob.glob(\"/notebooks/data/stores/*h5\")\n",
    "for storepath in stores:\n",
    "    with pd.HDFStore(storepath, mode='r', complevel=9, complib='blosc') as store:\n",
    "        print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Todo\n",
    "\n",
    "- Check for duplicates in \"successful-counts.txt\". I caught one text duplicated due to a bug, good to check that it doesn't happen again.\n",
    "- Is the uint32 size enough (0 - 4294967295) once I start merging? **NO** - `the` had 35 billion occurances in the PD dataset. \n",
    "- Create a table index after storage (e.g. `store.create_table_index('df', optlevel=9, kind='full')`)\n",
    "\n",
    "## Notes\n",
    "If a word doesn't occur at least twice in a batch of 400 texts (i.e. 2/180m words), it is trimmed from that batch. This cuts the size considerably.\n",
    "\n",
    "## Timing\n",
    "\n",
    "50/chunk, 20 batches: 5.40s/it / 1m48 for 1000texts\n",
    "50/chunk, 20 batches, no doc saving: 4.68s/it / 1m33s for 1000texts (seems fine to keep around?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
