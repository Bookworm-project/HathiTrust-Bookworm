{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Step 1: Processing Feature Files for Bookworm\n",
    "\n",
    "This notebook runs through Extracted Features files, saving:\n",
    "\n",
    "1. Global token counts (by language) toward the eventual Bookworm Wordlist. \n",
    "    These aren't all folded here: rather, they are folded by batch and saved to and HDF5 Store.\n",
    "    Later, they'll all be folded into one big list.\n",
    "\n",
    "2. \"Raw\" unigram counts per book. These will eventually be trimmed to only the BW vocabulary and\n",
    "    labelled by an id. This information first needs the wordlist that #1 above will create, but\n",
    "    since we're already opening the EF files, might as well do some processing and save this\n",
    "    intermediate state to a fast IO format (HDF5 store, again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook # Progress bars!\n",
    "from ipyparallel import Client\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before attaching to ipyparallel engines, they need to be started with \n",
    "\n",
    "```bash\n",
    "    ipcluster start -n NUM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = Client()\n",
    "dview = rc[:]\n",
    "v = rc.load_balanced_view()\n",
    "\n",
    "# Need this later to force garbage collection\n",
    "@dview.remote(block=True)\n",
    "def force_gc():\n",
    "    import gc\n",
    "    before = gc.get_count()\n",
    "    gc.collect()\n",
    "    return before[0], gc.get_count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize logging. There's no nice way to pass logs between engines, so just give each one its own log.\n",
    "\n",
    "The timestamp format is designed for easy sort, so you can track all logs with \n",
    "\n",
    "```bash\n",
    "watch \"tail -q -n 100 logs/* | sort\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Log initialized\n"
     ]
    }
   ],
   "source": [
    "def init_log(name=False):\n",
    "    import logging, os\n",
    "    if not name:\n",
    "        name = os.getpid()\n",
    "    handler = logging.FileHandler(\"/notebooks/data/logs/bw-%s.log\" % name, 'a')\n",
    "    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s', \"%m/%d-%H:%M:%S\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logging.info(\"Log initialized\")\n",
    "\n",
    "dview.push(dict(init_log=init_log))\n",
    "init_log(\"root\")\n",
    "%px init_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load paths to feature files. This notebook maintains a list of successully processed ids, so there are some functions that help us cross reference all volumes with done volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts 1533959\n"
     ]
    }
   ],
   "source": [
    "with open(\"/notebooks/features/listing/ucw-to-yul-full.1.txt\", \"r\") as f:\n",
    "    paths = [\"/notebooks/features/\"+path.strip() for path in f.readlines()][1:]\n",
    "    print(\"Number of texts\", len(paths))\n",
    "\n",
    "successfile = \"/notebooks/data/successful-counts.txt\"\n",
    "def get_processed():\n",
    "    import numpy as np\n",
    "    ''' Get already processed files. Wrapped in func for easy refresh'''\n",
    "    try:\n",
    "        with open(successfile, \"r\") as f:\n",
    "            paths = f.read().strip().split(\"\\n\")\n",
    "        paths = [\"/notebooks/features/\"+utils.id_to_rsync(path) for path in paths]\n",
    "        return np.array(paths)\n",
    "    except:\n",
    "        return np.array([])\n",
    "\n",
    "path_to_id = lambda x: x.replace(\".json.bz2\", \"\").split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_count` is the function that does the processing of the volume. To improve performance, however, the subprocesses run larger volumes in larger batches with `get_doc_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">heb</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">uc1.b3571708</th>\n",
       "      <th>!</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!00וזז־ו</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!09</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                count\n",
       "language id           token          \n",
       "heb      uc1.b3571708 !           150\n",
       "                      !00וזז־ו      1\n",
       "                      !09           1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trim_token(t, max=50):\n",
    "    ''' Trim unicode string to max number of bytes'''\n",
    "    if len(t.encode('utf-8')) > max:\n",
    "        while len(t.encode('utf-8')) > max:\n",
    "            t = t[:-1]\n",
    "    return t\n",
    "\n",
    "def get_count(path, store=False):\n",
    "    ''' Get tokencount information from a single doc, by path'''\n",
    "    from htrc_features import FeatureReader    \n",
    "    max_char = 50\n",
    "    vol = FeatureReader(path).first()\n",
    "    tl = vol.tokenlist(pages=False, pos=False)\n",
    "    if tl.empty:\n",
    "        return tl\n",
    "    else:\n",
    "        tl = tl.reset_index('section')[['count']]\n",
    "    tl.index = [trim_token(t, max_char) for t in tl.index.values]\n",
    "    tl.index.names=['token']\n",
    "    tl['id'] = vol.id\n",
    "    tl['language'] = vol.language\n",
    "    tl = tl.reset_index('token').set_index(['language', 'id', 'token']).sort_index()\n",
    "    return tl\n",
    "\n",
    "# Send to Engines\n",
    "dview.push(dict(trim_token=trim_token, get_count=get_count))\n",
    "\n",
    "# Example\n",
    "get_count(paths[0]).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_counts(paths, mincount=False, max_str_bytes = 50):\n",
    "    '''\n",
    "    This method lets you process multiple paths at a time on a single engine.\n",
    "    This means the engine can collect enough texts to do a simple filter (i.e. >X counts in Y texts)\n",
    "    and can save to it's own store.\n",
    "    '''\n",
    "    import logging\n",
    "    import os\n",
    "    import gc\n",
    "    import pandas as pd\n",
    "    fname = '/notebooks/data/stores/bw_counts_%s.h5' % os.getpid()\n",
    "    success_log = []\n",
    "    logging.info(\"Starting %d volume batch on PID=%s\" % (len(paths), os.getpid()))\n",
    "    with pd.HDFStore(fname, mode=\"a\", complevel=9, complib='blosc') as store:\n",
    "        tl_collector = []\n",
    "        for path in paths:\n",
    "            try:\n",
    "                tl = get_count(path, store=store)\n",
    "                if tl.empty:\n",
    "                    logging.info(\"%s is empty\" % path)\n",
    "                    continue\n",
    "                tl_collector.append(tl)\n",
    "            except:\n",
    "                logging.exception(\"Unable to get count for path %s\" % path)\n",
    "                continue\n",
    "            success_log.append(path)\n",
    "\n",
    "        # Save a DF combining all the counts from this batch\n",
    "        try:\n",
    "            logging.info(\"Merging and Saving texts for %d paths starting with %s\" % (len(paths), paths[0]))\n",
    "            combineddf = pd.concat(tl_collector)\n",
    "            \n",
    "            # Save tf(doc) with volid but no lang\n",
    "            # For efficient HDF5 storage, enforcing a 50 byte token limit. Can't use\n",
    "            # DataFrame.str.slice(stop=50) though, because really we care about bytes and \n",
    "            # some unicode chars are multiple codepoints.\n",
    "            # volids are capped at 25chars (the longest PD vol id)\n",
    "            store.append('tf/docs',\n",
    "                         combineddf.reset_index('language')[['count']],\n",
    "                         min_itemsize = {'id': 25, 'token':max_str_bytes})\n",
    "            \n",
    "            ### Save tf(corpus)\n",
    "            df = combineddf.groupby(level=['language', 'token'])[['count']]\\\n",
    "                           .sum().sort_index()\n",
    "            # Filtering this way (by corpus total, not language total) is too slow:\n",
    "            #if mincount:\n",
    "            #    df = df.groupby(level='token')[['count']].filter(lambda x: x.sum()>=mincount)\n",
    "            # Because we can't feasibly filter on total count and have to do so by lang x token, it\n",
    "            # might unfairly punish sparse languages. My workaround is to only even trim English by\n",
    "            # mincount: any bias this would have would be in the bottom of the wordlist anyway.\n",
    "            if mincount:\n",
    "                df = df[(df.index.get_level_values(0) != 'eng') | (df['count']>2)]\n",
    "            store.append('tf/corpus', df, min_itemsize = {'token': max_str_bytes})\n",
    "            tl_collector = dict()\n",
    "            return success_log\n",
    "        except:\n",
    "            logging.exception(\"Saving error for %d paths starting with %s\" % (len(paths), paths[0]))\n",
    "            return []\n",
    "    gc.collect()\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting parallel job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1533959 paths remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Done processing batch 1, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/02/z2/3d/0f/ark+=13960=t02z23d0f/ufl1.ark+=13960=t02z23d0f.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/0v/q3/w3/5f/ark+=13960=t0vq3w35f/ufl1.ark+=13960=t0vq3w35f.json.bz2\n",
      "INFO:root:Done processing batch 2, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/0v/q3/wk/1s/ark+=13960=t0vq3wk1s/ufl1.ark+=13960=t0vq3wk1s.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/1p/g2/nx/6q/ark+=13960=t1pg2nx6q/ufl1.ark+=13960=t1pg2nx6q.json.bz2\n",
      "INFO:root:Done processing batch 3, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/43/r2/0n/0p/ark+=13960=t43r20n0p/ufl1.ark+=13960=t43r20n0p.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/4z/g7/mw/81/ark+=13960=t4zg7mw81/ufl1.ark+=13960=t4zg7mw81.json.bz2\n",
      "INFO:root:Done processing batch 4, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/1p/g2/q1/6w/ark+=13960=t1pg2q16w/ufl1.ark+=13960=t1pg2q16w.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/2h/71/g1/9c/ark+=13960=t2h71g19c/ufl1.ark+=13960=t2h71g19c.json.bz2\n",
      "INFO:root:Done processing batch 5, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/2h/71/gj/74/ark+=13960=t2h71gj74/ufl1.ark+=13960=t2h71gj74.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/39/03/81/66/ark+=13960=t39038166/ufl1.ark+=13960=t39038166.json.bz2\n",
      "INFO:root:Done processing batch 6, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/6n/z9/7h/8w/ark+=13960=t6nz97h8w/ufl1.ark+=13960=t6nz97h8w.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/7g/q7/vc/45/ark+=13960=t7gq7vc45/ufl1.ark+=13960=t7gq7vc45.json.bz2\n",
      "INFO:root:Done processing batch 7, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/4z/g7/p4/9g/ark+=13960=t4zg7p49g/ufl1.ark+=13960=t4zg7p49g.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/5t/73/82/3t/ark+=13960=t5t73823t/ufl1.ark+=13960=t5t73823t.json.bz2\n",
      "INFO:root:Done processing batch 8, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/5t/73/89/3z/ark+=13960=t5t73893z/ufl1.ark+=13960=t5t73893z.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/6n/z9/7h/7d/ark+=13960=t6nz97h7d/ufl1.ark+=13960=t6nz97h7d.json.bz2\n",
      "INFO:root:Done processing batch 9, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/7g/q7/vd/15/ark+=13960=t7gq7vd15/ufl1.ark+=13960=t7gq7vd15.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/89/g6/sc/0b/ark+=13960=t89g6sc0b/ufl1.ark+=13960=t89g6sc0b.json.bz2\n",
      "INFO:root:Done processing batch 10, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/94/75/fv/9r/ark+=13960=t9475fv9r/ufl1.ark+=13960=t9475fv9r.json.bz2 to /notebooks/features/ufl2/pairtree_root/aa/00/01/16/32/_0/00/01/aa00011632_00001/ufl2.aa00011632_00001.json.bz2\n",
      "INFO:root:Done processing batch 11, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/39/03/82/74/ark+=13960=t39038274/ufl1.ark+=13960=t39038274.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/43/r2/0g/1x/ark+=13960=t43r20g1x/ufl1.ark+=13960=t43r20g1x.json.bz2\n",
      "INFO:root:Done processing batch 12, from /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/89/g6/sc/48/ark+=13960=t89g6sc48/ufl1.ark+=13960=t89g6sc48.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/94/75/ft/4w/ark+=13960=t9475ft4w/ufl1.ark+=13960=t9475ft4w.json.bz2\n",
      "INFO:root:Done processing batch 13, from /notebooks/features/uiuc/pairtree_root/68/76/15/5/6876155/uiuc.6876155.json.bz2 to /notebooks/features/uiuc/pairtree_root/71/06/53/3/7106533/uiuc.7106533.json.bz2\n",
      "INFO:root:Done processing batch 14, from /notebooks/features/uiuc/pairtree_root/71/06/99/6/7106996/uiuc.7106996.json.bz2 to /notebooks/features/uiuc/pairtree_root/74/03/99/2/7403992/uiuc.7403992.json.bz2\n",
      "INFO:root:Done processing batch 15, from /notebooks/features/uiuc/pairtree_root/56/04/32/3_/00/1/5604323_001/uiuc.5604323_001.json.bz2 to /notebooks/features/uiuc/pairtree_root/68/76/15/4/6876154/uiuc.6876154.json.bz2\n",
      "INFO:root:Done processing batch 16, from /notebooks/features/uiuc/pairtree_root/21/74/69/2v/5i/8/2174692v5i8/uiuc.2174692v5i8.json.bz2 to /notebooks/features/uiuc/pairtree_root/32/41/19/5_/00/1/3241195_001/uiuc.3241195_001.json.bz2\n",
      "ERROR:root:Problem with result in batch 17\n",
      "INFO:root:Done processing batch 18, from /notebooks/features/ufl2/pairtree_root/aa/00/01/16/33/_0/00/01/aa00011633_00001/ufl2.aa00011633_00001.json.bz2 to /notebooks/features/uiuc/pairtree_root/21/74/69/2v/5i/7/2174692v5i7/uiuc.2174692v5i7.json.bz2\n",
      "INFO:root:Done processing batch 19, from /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/8k/d2/7w/8k/ark+=13960=t8kd27w8k/ucw.ark+=13960=t8kd27w8k.json.bz2 to /notebooks/features/ufl1/pairtree_root/ar/k+/=1/39/60/=t/02/z2/32/99/ark+=13960=t02z23299/ufl1.ark+=13960=t02z23299.json.bz2\n",
      "INFO:root:Done processing batch 20, from /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/00/00/hc/4v/ark+=13960=t0000hc4v/ucw.ark+=13960=t0000hc4v.json.bz2 to /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/1n/g5/hc/8p/ark+=13960=t1ng5hc8p/ucw.ark+=13960=t1ng5hc8p.json.bz2\n",
      "INFO:root:Done processing batch 21, from /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/3c/z3/hf/96/ark+=13960=t3cz3hf96/ucw.ark+=13960=t3cz3hf96.json.bz2 to /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/53/f5/0z/3g/ark+=13960=t53f50z3g/ucw.ark+=13960=t53f50z3g.json.bz2\n",
      "INFO:root:Done processing batch 22, from /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/53/f5/10/1c/ark+=13960=t53f5101c/ucw.ark+=13960=t53f5101c.json.bz2 to /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/6t/x3/gj/85/ark+=13960=t6tx3gj85/ucw.ark+=13960=t6tx3gj85.json.bz2\n",
      "INFO:root:Done processing batch 23, from /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/1n/g5/jc/83/ark+=13960=t1ng5jc83/ucw.ark+=13960=t1ng5jc83.json.bz2 to /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/3c/z3/g8/44/ark+=13960=t3cz3g844/ucw.ark+=13960=t3cz3g844.json.bz2\n",
      "INFO:root:Done processing batch 24, from /notebooks/features/uiuc/pairtree_root/74/05/27/4/7405274/uiuc.7405274.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/00/58/08/83/30112000580883/uiug.30112000580883.json.bz2\n",
      "INFO:root:Done processing batch 25, from /notebooks/features/uiug/pairtree_root/30/11/20/00/58/17/09/30112000581709/uiug.30112000581709.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/00/93/21/26/30112000932126/uiug.30112000932126.json.bz2\n",
      "INFO:root:Done processing batch 26, from /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/6t/x3/hj/54/ark+=13960=t6tx3hj54/ucw.ark+=13960=t6tx3hj54.json.bz2 to /notebooks/features/ucw/pairtree_root/ar/k+/=1/39/60/=t/8k/d2/4z/5s/ark+=13960=t8kd24z5s/ucw.ark+=13960=t8kd24z5s.json.bz2\n",
      "INFO:root:Done processing batch 27, from /notebooks/features/uiug/pairtree_root/30/11/20/02/56/13/11/30112002561311/uiug.30112002561311.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/02/84/22/57/30112002842257/uiug.30112002842257.json.bz2\n",
      "INFO:root:Done processing batch 28, from /notebooks/features/uiug/pairtree_root/30/11/20/04/63/05/93/30112004630593/uiug.30112004630593.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/04/90/23/64/30112004902364/uiug.30112004902364.json.bz2\n",
      "INFO:root:Done processing batch 29, from /notebooks/features/uiug/pairtree_root/30/11/20/02/84/25/05/30112002842505/uiug.30112002842505.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/03/24/31/82/30112003243182/uiug.30112003243182.json.bz2\n",
      "INFO:root:Done processing batch 30, from /notebooks/features/uiug/pairtree_root/30/11/20/01/99/20/12/30112001992012/uiug.30112001992012.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/02/56/13/03/30112002561303/uiug.30112002561303.json.bz2\n",
      "INFO:root:Done processing batch 31, from /notebooks/features/uiug/pairtree_root/30/11/20/00/93/21/67/30112000932167/uiug.30112000932167.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/01/35/05/75/30112001350575/uiug.30112001350575.json.bz2\n",
      "INFO:root:Done processing batch 32, from /notebooks/features/uiug/pairtree_root/30/11/20/01/62/51/90/30112001625190/uiug.30112001625190.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/01/99/18/24/30112001991824/uiug.30112001991824.json.bz2\n",
      "INFO:root:Done processing batch 33, from /notebooks/features/uiug/pairtree_root/30/11/20/03/53/22/53/30112003532253/uiug.30112003532253.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/04/04/22/45/30112004042245/uiug.30112004042245.json.bz2\n",
      "INFO:root:Done processing batch 34, from /notebooks/features/uiug/pairtree_root/30/11/20/03/24/40/65/30112003244065/uiug.30112003244065.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/03/53/22/04/30112003532204/uiug.30112003532204.json.bz2\n",
      "INFO:root:Done processing batch 35, from /notebooks/features/uiug/pairtree_root/30/11/20/01/35/05/91/30112001350591/uiug.30112001350591.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/01/62/51/17/30112001625117/uiug.30112001625117.json.bz2\n",
      "INFO:root:Done processing batch 36, from /notebooks/features/uiug/pairtree_root/30/11/20/04/04/23/69/30112004042369/uiug.30112004042369.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/04/34/07/48/30112004340748/uiug.30112004340748.json.bz2\n",
      "INFO:root:Done processing batch 37, from /notebooks/features/uiug/pairtree_root/30/11/20/04/34/11/00/30112004341100/uiug.30112004341100.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/04/63/04/52/30112004630452/uiug.30112004630452.json.bz2\n",
      "INFO:root:Done processing batch 38, from /notebooks/features/uiug/pairtree_root/30/11/20/05/64/98/73/30112005649873/uiug.30112005649873.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/07/10/85/06/30112007108506/uiug.30112007108506.json.bz2\n",
      "INFO:root:Done processing batch 39, from /notebooks/features/uiug/pairtree_root/30/11/20/04/90/38/59/30112004903859/uiug.30112004903859.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/05/44/35/90/30112005443590/uiug.30112005443590.json.bz2\n",
      "INFO:root:Done processing batch 40, from /notebooks/features/uiug/pairtree_root/30/11/20/05/44/36/16/30112005443616/uiug.30112005443616.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/05/64/98/16/30112005649816/uiug.30112005649816.json.bz2\n",
      "INFO:root:Done processing batch 41, from /notebooks/features/uiug/pairtree_root/30/11/20/08/52/16/81/30112008521681/uiug.30112008521681.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/08/65/02/09/30112008650209/uiug.30112008650209.json.bz2\n",
      "INFO:root:Done processing batch 42, from /notebooks/features/uiug/pairtree_root/30/11/20/07/10/85/14/30112007108514/uiug.30112007108514.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/07/46/86/60/30112007468660/uiug.30112007468660.json.bz2\n",
      "INFO:root:Done processing batch 43, from /notebooks/features/uiug/pairtree_root/30/11/20/07/47/16/98/30112007471698/uiug.30112007471698.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/07/66/60/65/30112007666065/uiug.30112007666065.json.bz2\n",
      "INFO:root:Done processing batch 44, from /notebooks/features/uiug/pairtree_root/30/11/20/07/80/03/83/30112007800383/uiug.30112007800383.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/07/82/42/84/30112007824284/uiug.30112007824284.json.bz2\n",
      "INFO:root:Done processing batch 45, from /notebooks/features/uiug/pairtree_root/30/11/20/07/66/60/73/30112007666073/uiug.30112007666073.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/07/80/03/75/30112007800375/uiug.30112007800375.json.bz2\n",
      "INFO:root:Done processing batch 46, from /notebooks/features/uiug/pairtree_root/30/11/20/07/86/30/43/30112007863043/uiug.30112007863043.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/07/92/35/40/30112007923540/uiug.30112007923540.json.bz2\n",
      "INFO:root:Done processing batch 47, from /notebooks/features/uiug/pairtree_root/30/11/20/07/82/43/91/30112007824391/uiug.30112007824391.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/07/86/30/27/30112007863027/uiug.30112007863027.json.bz2\n",
      "INFO:root:Done processing batch 48, from /notebooks/features/uiug/pairtree_root/30/11/20/08/43/94/62/30112008439462/uiug.30112008439462.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/08/52/16/73/30112008521673/uiug.30112008521673.json.bz2\n",
      "INFO:root:Done processing batch 49, from /notebooks/features/uiug/pairtree_root/30/11/20/08/31/95/08/30112008319508/uiug.30112008319508.json.bz2 to /notebooks/features/uiug/pairtree_root/30/11/20/08/43/94/54/30112008439454/uiug.30112008439454.json.bz2\n"
     ]
    },
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: get_doc_counts\n[Engine Exception]EngineError: Engine b'f2d8ec3e-c791-41b2-b504-d2aacfdc2eae' died while running task '0547da2d-ca52-42c1-9a14-f66860442f4d'",
     "output_type": "error",
     "traceback": [
      "[Engine Exception]",
      "Traceback (most recent call last):",
      "  File \"/opt/conda/lib/python3.4/site-packages/ipyparallel/controller/scheduler.py\", line 347, in handle_stranded_tasks",
      "    raise error.EngineError(\"Engine %r died while running task %r\"%(engine, msg_id))",
      "ipyparallel.error.EngineError: Engine b'f2d8ec3e-c791-41b2-b504-d2aacfdc2eae' died while running task '0547da2d-ca52-42c1-9a14-f66860442f4d'",
      ""
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Split paths into N-sized chunks, so engines can iterate on multiple texts at once\n",
    "chunk_size = 800\n",
    "remaining_paths = np.setdiff1d(paths, get_processed())\n",
    "print(\"%d paths remaining\" % len(remaining_paths))\n",
    "n = 10000000\n",
    "start = 0\n",
    "chunked_paths = [remaining_paths[start+i:start+i+chunk_size] for i in range(0, len(remaining_paths[start:start+n]), chunk_size)]\n",
    "\n",
    "starttime = time.time()\n",
    "logging.info(\"Starting parallel job\")\n",
    "parallel_job = v.map(get_doc_counts, chunked_paths, ordered=False)\n",
    "\n",
    "i = 0\n",
    "for result in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    i += 1\n",
    "    if result:\n",
    "        with open(successfile, \"a+\") as f:\n",
    "            ids = [path_to_id(path) for path in result]\n",
    "            f.write(\"\\n\".join(ids)+\"\\n\")\n",
    "        logging.info(\"Done processing batch %d, from %s to %s\" % (i, result[0], result[-1]))\n",
    "    else:\n",
    "        logging.error(\"Problem with result in batch %d\" % i)\n",
    "\n",
    "force_gc()\n",
    "logging.info(\"Done\")\n",
    "logging.info(time.time()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Todo\n",
    "\n",
    "- Check for duplicates in \"successful-counts.txt\". I caught one text duplicated due to a bug, good to check that it doesn't happen again.\n",
    "- Create a table index after storage (e.g. `store.create_table_index('df', optlevel=9, kind='full')`)\n",
    "\n",
    "## Notes\n",
    "- Future merges need to be at uint64 or int64, because uint32 is too small. For some reason, PyTables doesn't allow uint64 data columns, so int64 is used solely for that reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "## Count up unique volume ids from stores\n",
    "\n",
    "Useful in the case I ran into where the ZMQ connect between the root and nodes broke, so I wasn't saving the list of successfully processed volumes, but the Engines were still happily crunching away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-df54f3a9d929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstorestocheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/notebooks/data/stores/*h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_for_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_processed' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from ipyparallel import require\n",
    "storestocheck = glob.glob(\"/notebooks/data/stores/*h5\")\n",
    "\n",
    "@require(get_processed)\n",
    "def check_for_processed(storefile):\n",
    "    import gc\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    from htrc_features import utils\n",
    "    \n",
    "    all_unique = []\n",
    "    batchsize = 100000000\n",
    "    \n",
    "    with pd.HDFStore(storefile, mode=\"r\") as store:\n",
    "        # Rejecting files where the last row was not mdp\n",
    "        try:\n",
    "            n = int(store.get_storer(\"/tf/docs\").nrows)\n",
    "        except:\n",
    "            logging.exception(\"Can't get row count for %s, moving on\" % storefile)\n",
    "            return []\n",
    "        try:\n",
    "            a = store.select_column('/tf/docs', 'id', start=n-2)\n",
    "            if a.str.split(\".\")[0][0] != 'mdp':\n",
    "                logging.info(\"%s didn't process mdp most recently, skipping.\" % storefile)\n",
    "                return []\n",
    "        except:\n",
    "            logging.exception(\"Error with %s\" % storefile)\n",
    "            return []\n",
    "\n",
    "        logging.info(\"Figuring out what is already processed.\")\n",
    "        already_processed = get_processed()\n",
    "\n",
    "        logging.info(\"Going through file backwards until all the volume ids are in the success list\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                logging.info(\"Processing %s from %d\" % (storefile, n-batchsize))\n",
    "                startrow = (n - batchsize) if n > batchsize else 0\n",
    "                unique = store.select_column('/tf/docs', 'id', start=startrow, stop=n).unique()\n",
    "                uniquemdp = unique[np.char.startswith(unique.astype(np.unicode), \"mdp\")]\n",
    "                as_paths =  pd.Series(uniquemdp).apply(lambda x: '/notebooks/features/' + utils.id_to_rsync(x)).values\n",
    "                \n",
    "                to_process = np.setdiff1d(as_paths, already_processed)\n",
    "                if to_process.shape[0] == 0:\n",
    "                    logging.info(\"Done at %d\" % (n-batchsize))\n",
    "                    break\n",
    "                else:\n",
    "                    n -= batchsize\n",
    "                    all_unique.append(to_process)\n",
    "            except:\n",
    "                n -= batchsize\n",
    "                logging.exception(\"Error with %s from %d)\" % (storefile, n))\n",
    "            try:\n",
    "                gc.collect()\n",
    "            except:\n",
    "                logging.exception(\"gc error\")\n",
    "    if len(all_unique) > 0:\n",
    "        try:\n",
    "            return np.unique(np.concatenate(all_unique))\n",
    "        except:\n",
    "            logging.exception(\"problem with array concatenatation, returning list\")\n",
    "            return all_unique\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick store check\n",
    "\n",
    "Grab the last item from each store. This is a good way to check if a store broke for whatever reason.\n",
    "\n",
    "The ptrepack command on your system seems to repack the non-corrupted part of the file, at least until it hits the error. That will be incomplete, but at least you have something that isn't crashing processes down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/data/stores/bw_counts_4796.h5\n",
      "0    uc1.b2990360\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4798.h5\n",
      "0    uc1.b2991200\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4815.h5\n",
      "0    uc1.b2993949\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4880.h5\n",
      "0    uc1.b2996411\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4862.h5\n",
      "0    uc1.b2997309\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4800.h5\n",
      "0    uc1.b2995580\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4802.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.4/site-packages/pandas/io/pytables.py:1988: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  values = values[self.cname]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    uc1.b2998141\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4782.h5\n",
      "0    uc1.b2999999\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4817.h5\n",
      "0    uc1.31822022962674\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4889.h5\n",
      "0    uc1.31822015234818\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4853.h5\n",
      "0    uc1.b2992047\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4844.h5\n",
      "0    uc1.b2993105\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4871.h5\n",
      "0    uc1.31822014309348\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4835.h5\n",
      "0    uc1.b2936728\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4907.h5\n",
      "0    uc1.b2939433\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4898.h5\n",
      "0    uc1.b2994756\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4826.h5\n",
      "0    uc1.b2935833\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4781.h5\n",
      "0    uc1.b2938538\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_4806.h5\n",
      "0    uc1.b2999047\n",
      "Name: id, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0    uc1.b2990360\n",
       " Name: id, dtype: object, 0    uc1.b2991200\n",
       " Name: id, dtype: object, 0    uc1.b2993949\n",
       " Name: id, dtype: object, 0    uc1.b2996411\n",
       " Name: id, dtype: object, 0    uc1.b2997309\n",
       " Name: id, dtype: object, 0    uc1.b2995580\n",
       " Name: id, dtype: object, 0    uc1.b2998141\n",
       " Name: id, dtype: object, 0    uc1.b2999999\n",
       " Name: id, dtype: object, 0    uc1.31822022962674\n",
       " Name: id, dtype: object, 0    uc1.31822015234818\n",
       " Name: id, dtype: object, 0    uc1.b2992047\n",
       " Name: id, dtype: object, 0    uc1.b2993105\n",
       " Name: id, dtype: object, 0    uc1.31822014309348\n",
       " Name: id, dtype: object, 0    uc1.b2936728\n",
       " Name: id, dtype: object, 0    uc1.b2939433\n",
       " Name: id, dtype: object, 0    uc1.b2994756\n",
       " Name: id, dtype: object, 0    uc1.b2935833\n",
       " Name: id, dtype: object, 0    uc1.b2938538\n",
       " Name: id, dtype: object, 0    uc1.b2999047\n",
       " Name: id, dtype: object]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "storestocheck = glob.glob(\"/notebooks/data/stores/*h5\")\n",
    "def get_last(storefile):\n",
    "    import pandas as pd\n",
    "    with pd.HDFStore(storefile, mode=\"a\") as store:\n",
    "        n = int(store.get_storer(\"/tf/docs\").nrows)\n",
    "        return store.select_column('/tf/docs', 'id', start=n-1)\n",
    "\n",
    "last = []\n",
    "for store in storestocheck:\n",
    "    print(store)\n",
    "    last.append(get_last(store))\n",
    "    print(last[-1])\n",
    "last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: check_for_processed\n[Engine Exception]EngineError: Engine b'e0665b72-c5bc-4726-9686-211e078b20b5' died while running task '4cbe26e4-5b12-46cd-ab24-535d307a1ffe'",
     "output_type": "error",
     "traceback": [
      "[Engine Exception]",
      "Traceback (most recent call last):",
      "  File \"/opt/conda/lib/python3.4/site-packages/ipyparallel/controller/scheduler.py\", line 347, in handle_stranded_tasks",
      "    raise error.EngineError(\"Engine %r died while running task %r\"%(engine, msg_id))",
      "ipyparallel.error.EngineError: Engine b'e0665b72-c5bc-4726-9686-211e078b20b5' died while running task '4cbe26e4-5b12-46cd-ab24-535d307a1ffe'",
      ""
     ]
    }
   ],
   "source": [
    "dview.push(dict(successfile=successfile, get_processed=get_processed))\n",
    "parallel_job = v.map(check_for_processed, storestocheck, ordered=False)\n",
    "all_ids = []\n",
    "i = 0\n",
    "for ids in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    all_ids.append(ids)\n",
    "    i += 1\n",
    "    logging.info(\"Batch %d done\" % i)\n",
    "\n",
    "uniqueids = np.unique(np.concatenate(all_ids))\n",
    "\n",
    "np.save(\"addtosuccessful2\", uniqueids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd.Series(uniqueids)\n",
    "b = a[a.str.find(\"mdp\") >= 0]\n",
    "c = get_processed()\n",
    "d = np.setdiff1d(b.values, c)\n",
    "e = pd.Series(d).apply(lambda x: x.split(\"/\")[-1].split(\".json\")[0]).values\n",
    "with open(successfile, \"a+\") as f:\n",
    "    f.write(\"\\n\".join(e)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1690746,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining_paths = np.setdiff1d(paths, get_processed())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
