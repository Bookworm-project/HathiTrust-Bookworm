{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Step 2: Counting up Global Counts for Word List\n",
    "\n",
    "In the EF processing script, token counts were collected in batches, folded to language x token counts in batches, and saved to HDF5 stores in `/store`. This script will fold those batches into a single list, so each language-token combination only has one count. The previous script was a `map`, this script with `reduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-banner\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"baa5e77a-853a-4933-b7ee-ece28df1bffb\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.11.1.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#baa5e77a-853a-4933-b7ee-ece28df1bffb\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipyparallel import Client\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "from bokeh.io import output_notebook\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler, visualize\n",
    "output_notebook()\n",
    "rawstores = glob.glob(\"/notebooks/data/batch2/stores/*h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Attach engines and initialize logging. *We'll be processing large in-memory chunks, so don't start too many processes.* I'm using a machine with 128MB RAM, and 10 processes hits around 2/3 of the RAM (80MB) for chunksize=1m in Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_log(name=False):\n",
    "    import logging, os\n",
    "    if not name:\n",
    "        name = os.getpid()\n",
    "    handler = logging.FileHandler(\"/notebooks/data2/logs/bw-%s.log\" % name, 'a')\n",
    "    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s', \"%m/%d-%H:%M:%S\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logging.info(\"Log initialized\")\n",
    "\n",
    "init_log('root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = Client()\n",
    "dview = rc[:]\n",
    "v = rc.load_balanced_view()\n",
    "\n",
    "# Need this later to force garbage collection\n",
    "@dview.remote(block=True)\n",
    "def force_gc():\n",
    "    import gc\n",
    "    before = gc.get_count()\n",
    "    gc.collect()\n",
    "    return before[0], gc.get_count()[0]\n",
    "\n",
    "dview.push(dict(init_log=init_log))\n",
    "%px init_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Triage and merge small chunks by lang (Parallelized)\n",
    "\n",
    "Iterate through all the stores, groupby by language then summing counts by token. These counts are still saved to an engine's own store under merge1/{language}, so that it can be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: _push>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def triage(inputstore):\n",
    "    try:\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import logging\n",
    "        import os\n",
    "        import gc\n",
    "    except:\n",
    "        return \"import error for \" + inputstore\n",
    "\n",
    "    chunksize = 1000000\n",
    "    storefolder = 'merged1' # this is in the h5 hierarchy\n",
    "    outputstorename = \"/notebooks/data2/batch2-redo/merge-%s.h5\" % os.getpid()\n",
    "    max_str_bytes = 50\n",
    "    \n",
    "    errors = 0\n",
    "    with pd.HDFStore(outputstorename, complevel=9, mode=\"a\", complib='blosc') as outstore:\n",
    "        with pd.HDFStore(inputstore, complevel=9, mode=\"r\", complib='blosc') as store:\n",
    "            row_size = store.get_storer('/tf/corpus').nrows\n",
    "            storeiter = store.select('/tf/corpus', start=0, chunksize=chunksize)\n",
    "\n",
    "            i = 0\n",
    "            for chunk in storeiter:\n",
    "                i += 1\n",
    "                try:\n",
    "                    lang_groups = chunk.groupby(level=['language'])\n",
    "                    for lang,df in lang_groups:\n",
    "                        if df.empty:\n",
    "                            continue\n",
    "                        merged = df.groupby(level=['token']).sum()\n",
    "                        \n",
    "                        fname = \"%s/%s\" % (storefolder, lang)\n",
    "                        outstore.append(fname, merged, data_columns=['count'], min_itemsize = {'index': max_str_bytes})\n",
    "                    logging.info(\"Completed %d/%d\" % (i, np.ceil(row_size/chunksize)))\n",
    "                except:\n",
    "                    errors += 1\n",
    "                    logging.exception(\"Error processing batch %d (docs %d-%d) of input store\" % (i, (i-1)*chunksize, i*chunksize))\n",
    "                gc.collect()\n",
    "    gc.collect()\n",
    "    if errors == 0:\n",
    "        return \"success\"\n",
    "    else:\n",
    "        return \"%d errors on process %s, check logs\" % (errors, os.getpid())\n",
    "dview.push(dict(triage=triage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed widget Javascript is the wrong version.\n"
     ]
    },
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: triage\n[Engine Exception]EngineError: Engine b'c426022b-5b16-44cd-a480-ca5d11c0d96e' died while running task 'd3269715-7885-49c3-806b-99d7af445991'",
     "output_type": "error",
     "traceback": [
      "[Engine Exception]",
      "Traceback (most recent call last):",
      "  File \"/opt/conda/lib/python3.5/site-packages/ipyparallel/controller/scheduler.py\", line 347, in handle_stranded_tasks",
      "    raise error.EngineError(\"Engine %r died while running task %r\"%(engine, msg_id))",
      "ipyparallel.error.EngineError: Engine b'c426022b-5b16-44cd-a480-ca5d11c0d96e' died while running task 'd3269715-7885-49c3-806b-99d7af445991'",
      ""
     ]
    }
   ],
   "source": [
    "logging.info(\"Processing Started\")\n",
    "parallel_job = v.map(triage, rawstores, ordered=False)\n",
    "i = 0\n",
    "\n",
    "for result in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    i += 1\n",
    "    if result == \"success\":\n",
    "        logging.info(\"Done processing batch %d\" % i)\n",
    "    else:\n",
    "        logging.error(result)\n",
    "        \n",
    "print(force_gc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Folding lang stores by bigger chunks (Parallelized)\n",
    "\n",
    "Starting with a blank DF, interate through each store (separately by language), selecting N million rows at once, and merging it into the initially blank DF (i.e. read chunk, concat to full_df, groupby(level='token'), and sum). Save to `/staged/{lang}`.\n",
    "\n",
    "Note that the engines are handed stores now, rather than saving to their own pid-named store.\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Might as well cycle through all of the chunks and collect to an in-memory collector. This will require a large amount of memory per process, given that the vocab can get to ~100m, and the process of append, group, sum will result in an in-memory copy. It's nonetheless much faster than any type of index-based iteration (trust Pandas and Numpy!), and a bit neater than running Step 2 repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stores = glob.glob(\"/notebooks/data3/fold/*h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fold(storefile, args):\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    gc.collect()\n",
    "    \n",
    "    storefolder = args['storefolder'] if 'storefolder' in args else '/staged'\n",
    "    # The PyTables folder to fold, as in \"folder/language\"\n",
    "    targetfolder = args['targetfolder'] if 'targetfolder' in args else '/merged1'\n",
    "    chunksize = args['chunksize'] if 'chunksize' in args else 1000000\n",
    "    max_str_bytes = 50\n",
    "    errors = 0\n",
    "    # Where queries\n",
    "    # These queries are cycled through, to optimize folding and to move to super-low occurance\n",
    "    # words (count == 1) aside to worry about later\n",
    "    # Note that this is only possible if count was saved as a data column earlier\n",
    "    (10)\n",
    "    default_wheres = [\"count>=%d and count <%d\" % nm for nm in [(10**6, 10**7), (10**5, 10**6), (10**4, 10**5),\n",
    "                                                    (1000, 10**4),(100, 1000), (50, 100), (10,50), (4,10)]]\n",
    "    default_wheres = [\"count >= %d\" % 10**7] + default_wheres + [\"count == %d\" % i for i in range(3,0, -1)]\n",
    "    wheres = args['wheres'] if 'wheres' in args else default_wheres\n",
    "    \n",
    "    with pd.HDFStore(storefile, complevel=9, mode=\"a\", complib='blosc') as store:\n",
    "        keys = [name for name in store.keys() if targetfolder in name]\n",
    "        for key in keys:\n",
    "            lang = key.split(\"/\")[-1]\n",
    "            logging.info(\"Processing %s in %s of %s\" % (lang, storefolder, storefile))\n",
    "            outfolder = \"%s/%s\" % (storefolder, lang)\n",
    "             # Output words with only one occurance to another table\n",
    "            #outfolder_ones = \"%s_ones/%s\" % (storefolder, lang)\n",
    "            row_size = store.get_storer(key).nrows\n",
    "            \n",
    "            for where in wheres:\n",
    "                i = 0\n",
    "                storeiter = store.select(key, where=where, chunksize=chunksize)\n",
    "                for chunk in storeiter:\n",
    "                    try:\n",
    "                        if chunk['count'].dtype != np.int64:\n",
    "                            chunk['count'] = chunk['count'].astype(\"int64\")\n",
    "                        folded = chunk.groupby(level='token').sum()\n",
    "                        store.append(outfolder,\n",
    "                                     folded,\n",
    "                                     data_columns=['count'], \n",
    "                                     min_itemsize = {'index': max_str_bytes})\n",
    "                        i += 1\n",
    "                        logging.info(\"Completed chunk %d for %s where %s\" % (i, lang, where))\n",
    "                        gc.collect()\n",
    "                    except:\n",
    "                        errors += 1\n",
    "                        logging.exception(\"Error folding %d/%d for %s\" % (i, np.ceil(row_size/chunksize), lang))\n",
    "    \n",
    "    gc.collect()\n",
    "    if errors == 0:\n",
    "        return \"success\"\n",
    "    else:\n",
    "        return \"%d folding errors on process %s, check logs\" % (errors, os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fold counts where count > 10**5. This allows a much bigger chunk size. We'll come back for the rest later\n",
    "#wheres = [\"count>=%d and count <%d\" % nm for nm in [(10**6, 10**7), (10**5, 10**6), (10**4, 10**5),\n",
    "#                                                    (1000, 10**4), (100, 1000), (50, 100), (10,50), (4,10)]]\n",
    "#wheres = [\"count >= %d\" % 10**7] + wheres\n",
    "wheres = [\"count>=%d and count <%d\" % nm for nm in [(1, 3)]]\n",
    "args = dict(targetfolder='/merged1', storefolder='/staged5-under3', chunksize=500000, wheres=wheres)\n",
    "parallel_job = v.map(fold, stores, [args]*len(stores), ordered=False)\n",
    "i = 0\n",
    "for result in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    i += 1\n",
    "    if result == \"success\":\n",
    "        logging.info(\"Done folding batch %d\" % i)\n",
    "    else:\n",
    "        logging.error(result)\n",
    "print(force_gc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100**5 to 10**6 took 29.56 with chunksize=1m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Final combine (Single process)\n",
    "\n",
    "Collect each lang's dfs from all the stores and merge. Easy-peasy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Everything below is incomplete.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/data2/batch2-redo/merge-3805.h5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Sort index as items are combined\n",
    "\n",
    "def sum_words(storefile, args):\n",
    "    '''\n",
    "    Merge entire tables into a final store\n",
    "    '''\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    \n",
    "    storefolder = args['storefolder'] if 'storefolder' in args else 'final'\n",
    "    targetfolder = args['targetfolder'] if 'targetfolder' in args else 'staged'\n",
    "    chunksize = args['chunksize'] if 'chunksize' in args else 4000000\n",
    "    max_str_bytes = 50\n",
    "    \n",
    "    with pd.HDFStore(storefile, complevel=9, mode=\"a\", complib='blosc') as store:\n",
    "        keys = [name for name in store.keys() if targetfolder in name]\n",
    "        for key in keys:\n",
    "            lang = key.split(\"/\")[-1]\n",
    "            collector = pd.DataFrame()\n",
    "            \n",
    "            outfolder = \"%s/%s\" % (storefolder, lang)\n",
    "            logging.info(\"Collecting final counts in %s of %s\" % (outfolder, storefile))\n",
    "            \n",
    "            i = 0\n",
    "            row_size = store.get_storer(key).nrows\n",
    "            storeiter = store.select(key, chunksize=chunksize)\n",
    "            for chunk in storeiter:\n",
    "                try:\n",
    "                    folded = chunk.groupby(level='token').sum().sort_index()\n",
    "                    store.append(outfolder, folded, min_itemsize = {'index': max_str_bytes})\n",
    "                    i += 1\n",
    "                    logging.info(\"Completed chunk %d/%d for %s\" % (i, np.ceil(row_size/chunksize), lang))\n",
    "                    gc.collect()\n",
    "                except:\n",
    "                    errors += 1\n",
    "                    logging.exception(\"Error folding %d/%d for %s\" % (i, np.ceil(row_size/chunksize), lang))\n",
    "    \n",
    "    gc.collect()\n",
    "    if errors == 0:\n",
    "        return \"success\"\n",
    "    else:\n",
    "        return \"%d folding errors on process %s, check logs\" % (errors, os.getpid())\n",
    "    \n",
    "args = dict(targetfolder='/staged5', storefolder='/final')\n",
    "sum_words(sumstores[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
