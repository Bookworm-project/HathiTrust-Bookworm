{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Step 2: Counting up Global Counts for Word List\n",
    "\n",
    "In the EF processing script, token counts were collected in batches, folded to language x token counts in batches, and saved to HDF5 stores in `/store`. This script will fold those batches into a single list, so each language-token combination only has one count. The previous script was a `map`, this script with `reduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipyparallel import Client\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "rawstores = glob.glob(\"/notebooks/data/batch2/stores/*h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Attach engines and initialize logging. *We'll be processing large in-memory chunks, so don't start too many processes.* I'm using a machine with 128MB RAM, and 10 processes hits around 2/3 of the RAM (80MB) for chunksize=1m in Step 1. Use many fewer processes for step 2 (todo: add what *many fewer* means!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rc = Client()\n",
    "dview = rc[:]\n",
    "v = rc.load_balanced_view()\n",
    "\n",
    "# Need this later to force garbage collection\n",
    "@dview.remote(block=True)\n",
    "def force_gc():\n",
    "    import gc\n",
    "    before = gc.get_count()\n",
    "    gc.collect()\n",
    "    return before[0], gc.get_count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_log(name=False):\n",
    "    import logging, os\n",
    "    if not name:\n",
    "        name = os.getpid()\n",
    "    handler = logging.FileHandler(\"/notebooks/data2/logs/bw-%s.log\" % name, 'a')\n",
    "    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s', \"%m/%d-%H:%M:%S\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logging.info(\"Log initialized\")\n",
    "\n",
    "dview.push(dict(init_log=init_log))\n",
    "%px init_log()\n",
    "init_log(\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Triage and merge small chunks by lang (Parallelized)\n",
    "\n",
    "Iterate through all the stores, groupby by language then summing counts by token. These counts are still saved to an engine's own store under merge1/{language}, so that it can be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: _push>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def triage(inputstore):\n",
    "    try:\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import logging\n",
    "        import os\n",
    "        import gc\n",
    "    except:\n",
    "        return \"import error for \" + inputstore\n",
    "\n",
    "    chunksize = 2000000\n",
    "    storefolder = 'merged1' # this is in the h5 hierarchy\n",
    "    outputstorename = \"/notebooks/data2/batch2/merge-%s.h5\" % os.getpid()\n",
    "    max_str_bytes = 50\n",
    "    \n",
    "    errors = 0\n",
    "    with pd.HDFStore(outputstorename, complevel=9, mode=\"a\", complib='blosc') as outstore:\n",
    "        with pd.HDFStore(inputstore, complevel=9, mode=\"r\", complib='blosc') as store:\n",
    "            row_size = store.get_storer('/tf/corpus').nrows\n",
    "            storeiter = store.select('/tf/corpus', start=0, chunksize=chunksize)\n",
    "\n",
    "            i = 0\n",
    "            for chunk in storeiter:\n",
    "                i += 1\n",
    "                try:\n",
    "                    lang_groups = chunk.groupby(level=['language'])\n",
    "                    for lang,df in lang_groups:\n",
    "                        if df.empty:\n",
    "                            continue\n",
    "                        merged = df.groupby(level=['token']).sum()\n",
    "                        fname = \"%s/%s\" % (storefolder, lang)\n",
    "                        outstore.append(fname, merged, min_itemsize = {'index': max_str_bytes})\n",
    "                    logging.info(\"Completed %d/%d\" % (i, np.ceil(row_size/chunksize)))\n",
    "                except:\n",
    "                    errors += 1\n",
    "                    logging.exception(\"Error processing batch %d (docs %d-%d) of input store\" % (i, (i-1)*chunksize, i*chunksize))\n",
    "                gc.collect()\n",
    "    gc.collect()\n",
    "    if errors == 0:\n",
    "        return \"success\"\n",
    "    else:\n",
    "        return \"%d errors on process %s, check logs\" % (errors, os.getpid())\n",
    "dview.push(dict(triage=triage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed widget Javascript is the wrong version.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Processing Started\")\n",
    "parallel_job = v.map(triage, rawstores, ordered=False)\n",
    "i = 0\n",
    "\n",
    "for result in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    i += 1\n",
    "    if result == \"success\":\n",
    "        logging.info(\"Done processing batch %d\" % i)\n",
    "    else:\n",
    "        logging.error(result)\n",
    "        \n",
    "print(force_gc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Folding lang stores by bigger chunks (Parallelized)\n",
    "\n",
    "Starting with a blank DF, interate through each store (separately by language), selecting N million rows at once, and merging it into the initially blank DF (i.e. read chunk, concat to full_df, groupby(level='token'), and sum). Save to `/staged/{lang}`.\n",
    "\n",
    "Note that the engines are handed stores now, rather than saving to their own pid-named store.\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Might as well cycle through all of the chunks and collect to an in-memory collector. This will require a large amount of memory per process, given that the vocab can get to ~100m, and the process of append, group, sum will result in an in-memory copy. It's nonetheless much faster than any type of index-based iteration (trust Pandas and Numpy!), and a bit neater than running Step 2 repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stores = glob.glob(\"/notebooks/data2/batch1/*h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fold(storefile, args):\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    gc.collect()\n",
    "    \n",
    "    storefolder = args['storefolder'] if 'storefolder' in args else '/staged'\n",
    "    # The PyTables folder to fold, as in \"folder/language\"\n",
    "    targetfolder = args['targetfolder'] if 'targetfolder' in args else '/merged1'\n",
    "    # Much bigger chunk size, because groupby().sum() will be much more effective now that\n",
    "    # languages are no longer mixed\n",
    "    chunksize = args['chunksize'] if 'chunksize' in args else 4000000\n",
    "    max_str_bytes = 50\n",
    "    \n",
    "    with pd.HDFStore(storefile, complevel=9, mode=\"a\", complib='blosc') as store:\n",
    "        keys = [name for name in store.keys() if targetfolder in name]\n",
    "        for key in keys:\n",
    "            lang = key.split(\"/\")[-1]\n",
    "            logging.info(\"Processing %s in %s of %s\" % (lang, storefolder, storefile))\n",
    "            outfolder = \"%s/%s\" % (storefolder, lang)\n",
    "            i = 0\n",
    "            row_size = store.get_storer(key).nrows\n",
    "            storeiter = store.select(key, chunksize=chunksize)\n",
    "            for chunk in storeiter:\n",
    "                try:\n",
    "                    folded = chunk.groupby(level='token').sum()\n",
    "                    store.append(outfolder, folded, min_itemsize = {'index': max_str_bytes})\n",
    "                    i += 1\n",
    "                    logging.info(\"Completed chunk %d/%d for %s\" % (i, np.ceil(row_size/chunksize), lang))\n",
    "                    gc.collect()\n",
    "                except:\n",
    "                    except:\n",
    "                    errors += 1\n",
    "                    logging.exception(\"Error folding %d/%d for %s\" % (i, np.ceil(row_size/chunksize), lang))\n",
    "    \n",
    "    gc.collect()\n",
    "    if errors == 0:\n",
    "        return \"success\"\n",
    "    else:\n",
    "        return \"%d folding errors on process %s, check logs\" % (errors, os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = dict(targetfolder='/merged1', storefolder='/staged')\n",
    "parallel_job = v.map(fold, rawstores, [args]*len(rawstores), ordered=False)\n",
    "i = 0\n",
    "\n",
    "for result in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    i += 1\n",
    "    if result == \"success\":\n",
    "        logging.info(\"Done folding batch %d\" % i)\n",
    "    else:\n",
    "        logging.error(result)\n",
    "        \n",
    "print(force_gc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = dict(targetfolder='/staged2', storefolder='/staged3')\n",
    "fold(storefile=stores[3], args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Delete intermediate tables\n",
    "\n",
    "At risk of running out of disk space? It's fine to delete `/merged1`. Sensibly, the size of this is only a tiny fraction of the per-volume counts: 100GB/1m texts, so it will be 1.5TB if not deleted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delete = False\n",
    "if delete:\n",
    "    for storename in rawstores:\n",
    "        with pd.HDFStore(storename, complevel=9, mode=\"a\", complib='blosc') as store:\n",
    "            keys = [name for name in store.keys() if \"/merged1\" in name]\n",
    "            for key in keys:\n",
    "                store.remove(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermission: Check Table Sizes\n",
    "\n",
    "See if it's actually folding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35149068, 3458987, 36154650, 35320506, 35570859, 2399990]\n",
      "[0, 0, 0, 34971494, 0, 0]\n",
      "[0, 0, 0, 16520678, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_total_size(storefile, args):\n",
    "    import pandas as pd\n",
    "    targetfolder = args['targetfolder']\n",
    "    nrows = 0\n",
    "    with pd.HDFStore(storefile, complevel=9, mode=\"a\", complib='blosc') as store:\n",
    "        keys = [name for name in store.keys() if targetfolder in name]\n",
    "        for key in keys:\n",
    "            nrows += store.get_storer(key).nrows\n",
    "    return nrows\n",
    "dview.push(dict(get_total_size=get_total_size))\n",
    "\n",
    "for name in ['/merged1', 'staged']:\n",
    "    print()\n",
    "    args = dict(targetfolder = name)\n",
    "    per_store = dview.map_sync(get_total_size, stores, [args]*len(stores))\n",
    "    print(\"Total rows in %s: %d\" % (name, sum(args)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Final combine (Single process)\n",
    "\n",
    "Collect each lang's dfs from all the stores and merge. Easy-peasy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Everything below is incomplete.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum(storefile, args):\n",
    "    '''\n",
    "    Merge entire tables into a final store\n",
    "    '''\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    \n",
    "    storefolder = args['storefolder'] if 'storefolder' in args else 'final'\n",
    "    targetfolder = args['targetfolder'] if 'targetfolder' in args else 'staged'\n",
    "    chunksize = args['chunksize'] if 'chunksize' in args else 4000000\n",
    "    max_str_bytes = 50\n",
    "    \n",
    "    with pd.HDFStore(storefile, complevel=9, mode=\"a\", complib='blosc') as store:\n",
    "        keys = [name for name in store.keys() if targetfolder in name]\n",
    "        for key in keys:\n",
    "            lang = key.split(\"/\")[-1]\n",
    "            collector = pd.DataFrame()\n",
    "            \n",
    "            outfolder = \"%s/%s\" % (storefolder, lang)\n",
    "            logging.info(\"Collecting final counts in %s of %s\" % (outfolder, storefile))\n",
    "            \n",
    "            i = 0\n",
    "            row_size = store.get_storer(key).nrows\n",
    "            storeiter = store.select(key, chunksize=chunksize)\n",
    "            for chunk in storeiter:\n",
    "                try:\n",
    "                    folded = chunk.groupby(level='token').sum()\n",
    "                    store.append(outfolder, folded, min_itemsize = {'index': max_str_bytes})\n",
    "                    i += 1\n",
    "                    logging.info(\"Completed chunk %d/%d for %s\" % (i, np.ceil(row_size/chunksize), lang))\n",
    "                    gc.collect()\n",
    "                except:\n",
    "                    except:\n",
    "                    errors += 1\n",
    "                    logging.exception(\"Error folding %d/%d for %s\" % (i, np.ceil(row_size/chunksize), lang))\n",
    "    \n",
    "    gc.collect()\n",
    "    if errors == 0:\n",
    "        return \"success\"\n",
    "    else:\n",
    "        return \"%d folding errors on process %s, check logs\" % (errors, os.getpid())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "5e7a175b3ffd4f9eb6c1c86c28f0027e": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
